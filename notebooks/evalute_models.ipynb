{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78eb6988",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "## Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44775eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import (\n",
    "    evaluate_all_saved_models,\n",
    "    evaluate_model,\n",
    "    evaluate_specific_models,\n",
    "    evaluate_manual_folders,\n",
    "    evaluate_from_experiment_configs\n",
    ")\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "from src.wandb_utils import *\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a184ea55",
   "metadata": {},
   "source": [
    "### Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9502ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.wandb_utils import login, check_wandb_mode, sync_offline_runs\n",
    "\n",
    "# \"online\", \"offline\", or \"disabled\"\n",
    "# If set to offlien dont forget to sink\n",
    "WANDB_MODE = \"offline\" \n",
    "\n",
    "print(\"Initializing Weights & Biases...\")\n",
    "current_mode = login(\n",
    "    project=\"emotion-classifier-vit\",\n",
    "    mode=WANDB_MODE\n",
    ")\n",
    "\n",
    "print(f\"W&B initialized successfully in {current_mode.upper()} mode!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494aa686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights and Biases Util Commands\n",
    "\n",
    "# Check current mode\n",
    "check_wandb_mode()\n",
    "\n",
    "# List available offline runs\n",
    "list_offline_runs()\n",
    "\n",
    "# Uncomment and use the commands below as needed:\n",
    "\n",
    "# Sync offline runs (when you have internet)\n",
    "# sync_offline_runs()\n",
    "\n",
    "# Change mode to online\n",
    "# set_wandb_mode(\"online\")\n",
    "\n",
    "# Clear offline runs (use with caution!)\n",
    "# clear_offline_runs(confirm=False)  # Dry run first\n",
    "# clear_offline_runs(confirm=True)   # Actually delete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7274a9d",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset for evaluation\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18afbb9",
   "metadata": {},
   "source": [
    "## Evaluate All Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4742a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üìä Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate ALL saved models in checkpoints directory\n",
    "print(\"üöÄ Evaluating all saved models...\")\n",
    "all_models_summary = evaluate_all_saved_models(test_ds)\n",
    "\n",
    "if all_models_summary:\n",
    "    best_model = all_models_summary[0]\n",
    "    print(f\"\\nüèÜ Overall Best Model: {best_model['experiment']}\")\n",
    "    print(f\"   Test Accuracy: {best_model['test_accuracy']:.4f}\")\n",
    "    print(f\"   Run Folder: {best_model['run_folder']}\")\n",
    "else:\n",
    "    print(\"‚ùå No models were successfully evaluated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773a072",
   "metadata": {},
   "source": [
    "## Selective Evaluation\n",
    "\n",
    "Evaluate specific experiments by name. The system will automatically find the latest run for each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b5d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which experiments to evaluate\n",
    "experiments_to_evaluate = [\n",
    "    # Add your experiment names here\n",
    "    # Example:\n",
    "    # \"baseline_none\",\n",
    "    # \"baseline_light\", \n",
    "    # \"baseline_medium\",\n",
    "    # \"baseline_heavy\"\n",
    "]\n",
    "\n",
    "if experiments_to_evaluate:\n",
    "    print(f\"üéØ Evaluating {len(experiments_to_evaluate)} specific experiments...\")\n",
    "    specific_summary = evaluate_specific_models(experiments_to_evaluate, test_ds)\n",
    "    \n",
    "    if specific_summary:\n",
    "        best_specific = specific_summary[0]\n",
    "        print(f\"\\nüèÜ Best among selected: {best_specific['experiment']}\")\n",
    "        print(f\"   Test Accuracy: {best_specific['test_accuracy']:.4f}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No experiments specified for selective evaluation\")\n",
    "    print(\"   Add experiment names to the 'experiments_to_evaluate' list above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04690e5e",
   "metadata": {},
   "source": [
    "## Detailed Individual Model Evaluation\n",
    "\n",
    "Evaluate specific models with detailed reporting and individual WandB runs for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08059b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Individual Model Evaluation\n",
    "\n",
    "# Define models to evaluate with display names\n",
    "MODELS_TO_EVALUATE = {\n",
    "    # \"Display Name\": \"checkpoint_folder_name\",\n",
    "    # Example:\n",
    "    # \"Baseline None Latest\": \"baseline_none\",\n",
    "    # \"Baseline Light v2\": \"baseline_light1\",\n",
    "    # \"Heavy Augmentation\": \"baseline_heavy\"\n",
    "}\n",
    "\n",
    "all_metrics = {}\n",
    "\n",
    "if MODELS_TO_EVALUATE:\n",
    "    print(f\"üîç Starting detailed evaluation of {len(MODELS_TO_EVALUATE)} models...\")\n",
    "    \n",
    "    for model_display_name, checkpoint_folder in MODELS_TO_EVALUATE.items():\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Evaluating: {model_display_name}\")\n",
    "        print(f\"Checkpoint: {checkpoint_folder}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        try:\n",
    "            # Find the latest run for this experiment\n",
    "            from src.evaluate import find_latest_run_for_experiment, load_model_from_checkpoint\n",
    "            \n",
    "            run_folder = find_latest_run_for_experiment(checkpoint_folder)\n",
    "            checkpoint_path = run_folder / f\"best_{run_folder.name}.pth\"\n",
    "            \n",
    "            if not checkpoint_path.exists():\n",
    "                print(f\"‚ùå Checkpoint not found: {checkpoint_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Load model\n",
    "            model = load_model_from_checkpoint(checkpoint_path)\n",
    "            \n",
    "            # Initialize W&B run for this evaluation (if online)\n",
    "            if get_wandb_mode() != \"disabled\":\n",
    "                import wandb\n",
    "                wandb.init(\n",
    "                    project=\"emotion-classification-eval\",\n",
    "                    name=f\"eval_{model_display_name}\",\n",
    "                    job_type=\"evaluation\",\n",
    "                    reinit=True\n",
    "                )\n",
    "            \n",
    "            # Evaluate with detailed logging\n",
    "            metrics = evaluate_model(\n",
    "                model=model, \n",
    "                test_dataset=test_ds,\n",
    "                log_to_wandb=(get_wandb_mode() != \"disabled\"),\n",
    "                run_name=model_display_name\n",
    "            )\n",
    "            \n",
    "            # Store metrics\n",
    "            all_metrics[model_display_name] = metrics\n",
    "            \n",
    "            # Print detailed report\n",
    "            from src.evaluate import print_classification_report\n",
    "            print_classification_report(metrics)\n",
    "            \n",
    "            # Finish W&B run\n",
    "            if get_wandb_mode() != \"disabled\" and wandb.run is not None:\n",
    "                wandb.finish()\n",
    "            \n",
    "            print(f\"‚úÖ Completed evaluation for {model_display_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to evaluate {model_display_name}: {e}\")\n",
    "            continue\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No models specified for detailed evaluation\")\n",
    "    print(\"   Add models to the 'MODELS_TO_EVALUATE' dictionary above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666a9ed",
   "metadata": {},
   "source": [
    "## Model Comparison Summary\n",
    "\n",
    "Compare all evaluated models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Summary\n",
    "if all_metrics:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for model_name, metrics in all_metrics.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
    "            'Precision': f\"{metrics['precision']:.4f}\",\n",
    "            'Recall': f\"{metrics['recall']:.4f}\",\n",
    "            'F1-Score': f\"{metrics['f1']:.4f}\"\n",
    "        })\n",
    "    \n",
    "    # Print formatted table\n",
    "    header = f\"{'Model':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "    \n",
    "    for row in comparison_data:\n",
    "        print(f\"{row['Model']:<25} {row['Accuracy']:<10} {row['Precision']:<10} {row['Recall']:<10} {row['F1-Score']:<10}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No models available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe1510",
   "metadata": {},
   "source": [
    "## Best Model Identification\n",
    "\n",
    "Identify the best performing model from the evaluated set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db85405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Model Identification\n",
    "if all_metrics:\n",
    "    # Find best by F1-score (you can change this to accuracy, precision, etc.)\n",
    "    best_model_name = max(all_metrics.items(), key=lambda x: x[1]['f1'])\n",
    "    best_by_accuracy = max(all_metrics.items(), key=lambda x: x[1]['accuracy'])\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"BEST MODEL IDENTIFICATION\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ BEST BY F1-SCORE: {best_model_name[0]}\")\n",
    "    print(f\"   F1-Score:  {best_model_name[1]['f1']:.4f}\")\n",
    "    print(f\"   Accuracy:  {best_model_name[1]['accuracy']:.4f}\")\n",
    "    print(f\"   Precision: {best_model_name[1]['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {best_model_name[1]['recall']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ BEST BY ACCURACY: {best_by_accuracy[0]}\")\n",
    "    print(f\"   Accuracy:  {best_by_accuracy[1]['accuracy']:.4f}\")\n",
    "    print(f\"   F1-Score:  {best_by_accuracy[1]['f1']:.4f}\")\n",
    "    print(f\"   Precision: {best_by_accuracy[1]['precision']:.4f}\")\n",
    "    print(f\"   Recall:    {best_by_accuracy[1]['recall']:.4f}\")\n",
    "    \n",
    "    # Show performance comparison\n",
    "    print(f\"\\nüìä PERFORMANCE RANGE:\")\n",
    "    all_f1 = [m['f1'] for m in all_metrics.values()]\n",
    "    all_acc = [m['accuracy'] for m in all_metrics.values()]\n",
    "    print(f\"   F1-Score:  {min(all_f1):.4f} - {max(all_f1):.4f}\")\n",
    "    print(f\"   Accuracy:  {min(all_acc):.4f} - {max(all_acc):.4f}\")\n",
    "    \n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c892523",
   "metadata": {},
   "source": [
    "## Performance Visualization\n",
    "\n",
    "Create visual comparisons of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1963a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Visualization\n",
    "if all_metrics and len(all_metrics) > 1:\n",
    "    print(\"\\nüìà Creating performance visualizations...\")\n",
    "    \n",
    "    model_names = list(all_metrics.keys())\n",
    "    accuracies = [all_metrics[name]['accuracy'] for name in model_names]\n",
    "    f1_scores = [all_metrics[name]['f1'] for name in model_names]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Accuracy comparison\n",
    "    bars1 = ax1.bar(model_names, accuracies, color='skyblue', alpha=0.8)\n",
    "    ax1.set_title('Model Accuracy Comparison')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # F1-Score comparison\n",
    "    bars2 = ax2.bar(model_names, f1_scores, color='lightcoral', alpha=0.8)\n",
    "    ax2.set_title('Model F1-Score Comparison')\n",
    "    ax2.set_ylabel('F1-Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Performance visualization saved as 'model_comparison.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion Classifier ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
