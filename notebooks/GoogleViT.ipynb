{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6402fff3",
   "metadata": {},
   "source": [
    "# Google ViT \n",
    "## Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 \n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from transformers import ViTForImageClassification\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from src.transforms import base_transform\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.config import (\n",
    "    DEVICE, \n",
    "    NUM_LABELS, \n",
    "    EMOTION_LABELS,\n",
    "    DEFAULT_BATCH_SIZE,\n",
    "    DEFAULT_LEARNING_RATE\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from src.train import train_model\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91481961",
   "metadata": {},
   "source": [
    "### Weights and Biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f21c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 \n",
    "from src.wandb_utils import login, check_wandb_mode, sync_offline_runs\n",
    "\n",
    "# \"online\", \"offline\", or \"disabled\"\n",
    "# If set to offlien dont forget to sink\n",
    "WANDB_MODE = \"online\" \n",
    "\n",
    "print(\"Initializing Weights & Biases...\")\n",
    "current_mode = login(\n",
    "    project=\"emotion-classifier-vit\",\n",
    "    mode=WANDB_MODE\n",
    ")\n",
    "\n",
    "print(f\"W&B initialized successfully in {current_mode.upper()} mode!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac93a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.wandb_utils import *\n",
    "\n",
    "# Weights and Biases Util Commands \n",
    "\n",
    "# Check current mode\n",
    "# check_wandb_mode()\n",
    "\n",
    "# Sync offline runs (when you have internet)\n",
    "sync_offline_runs()\n",
    "\n",
    "# List available offline runs\n",
    "# list_offline_runs()\n",
    "\n",
    "# Change mode \n",
    "# set_wandb_mode(\"online\")  \n",
    "\n",
    "# Set Confirm to False for a Dry Run\n",
    "# clear_offline_runs(confirm=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9719f4",
   "metadata": {},
   "source": [
    "---\n",
    "##  Fine Tuning Section\n",
    "Using FER2013 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebee14",
   "metadata": {},
   "source": [
    "### Tranformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simpler transformation sets without deprecated parameters\n",
    "transform_configs = {\n",
    "    \"none\": base_transform(),  # Use the base transforms from transforms.py\n",
    "    \n",
    "    \"light\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        A.Affine(translate_percent=0.05, scale=(0.95, 1.05), rotate=(-10, 10), p=0.3),\n",
    "        *base_transform()  # Include base transforms at the end\n",
    "    ]),\n",
    "    \n",
    "    \"medium\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Affine(translate_percent=0.1, scale=(0.9, 1.1), rotate=(-15, 15), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        *base_transform()  # Include base transforms at the end\n",
    "    ]),\n",
    "    \n",
    "    \"heavy\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Affine(translate_percent=0.15, scale=(0.85, 1.15), rotate=(-20, 20), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
    "        A.GridDropout(ratio=0.1, p=0.3),\n",
    "        *base_transform()  # Include base transforms at the end\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Transformation Configs Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6141",
   "metadata": {},
   "source": [
    "### Hyper Parameter Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "EPOCHS = 6\n",
    "\n",
    "experiment_configs = [\n",
    "    # Baseline with different transforms\n",
    "    {\n",
    "        \"name\": \"baseline_none\",\n",
    "        \"transform_key\": \"none\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_light\",\n",
    "        \"transform_key\": \"light\", \n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_medium\",\n",
    "        \"transform_key\": \"medium\",\n",
    "        \"epochs\": EPOCHS, \n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_heavy\",\n",
    "        \"transform_key\": \"heavy\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE, \n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"{len(experiment_configs)} Experiment Configs Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b0475",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results = {}\n",
    "\n",
    "for i, config in enumerate(tqdm(experiment_configs, desc=\"Training Experiments\")):\n",
    "    print(f\"\\n Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    print(f\"   Transform: {config['transform_key']}, LR: {config['learning_rate']}, Epochs: {config['epochs']}\")\n",
    "    \n",
    "    transform = transform_configs[config['transform_key']]\n",
    "    \n",
    "    train = FER2013Dataset(\n",
    "        split=\"train\",\n",
    "        transform=transform\n",
    "    )\n",
    "    valid = FER2013Dataset(\n",
    "        split=\"valid\", \n",
    "        transform=base_transform()  \n",
    "    )\n",
    "    \n",
    "    model = ViTForImageClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    model_exp, history_exp, run_folder_exp = train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_dataset=train,\n",
    "        val_dataset=valid,\n",
    "        num_epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        device=DEVICE,\n",
    "        model_name=config['name'],  \n",
    "        use_wandb=True,\n",
    "        wandb_config={\n",
    "            \"learning_rate\": config['learning_rate'],\n",
    "            \"batch_size\": config['batch_size'],\n",
    "            \"epochs\": config['epochs'],\n",
    "            \"weight_decay\": config['weight_decay'],\n",
    "            \"model_name\": \"vit_base_patch16_224\",\n",
    "            \"architecture\": \"ViT\", \n",
    "            \"dataset\": \"FER2013\",\n",
    "            \"transform_set\": config['transform_key'],\n",
    "            \"experiment_name\": config['name']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "    all_results[config['name']] = {\n",
    "        'model': model_exp,\n",
    "        'history': history_exp,\n",
    "        'run_folder': run_folder_exp,\n",
    "        'config': config,\n",
    "        'best_val_accuracy': max(history_exp['val_acc']),      \n",
    "        'best_val_loss': min(history_exp['val_loss']),\n",
    "        'final_train_accuracy': history_exp['train_acc'][-1],  \n",
    "        'final_train_loss': history_exp['train_loss'][-1]\n",
    "    }\n",
    "    \n",
    "    print(f\"   Completed: {config['name']}\")\n",
    "    print(f\"   Best Val Accuracy: {all_results[config['name']]['best_val_accuracy']:.4f}\")\n",
    "    print(f\"   Run folder: {run_folder_exp}\")\n",
    "    \n",
    "    # Clean up to free memory (optional but helpful)\n",
    "    del model_exp, optimizer\n",
    "    torch.cuda.empty_cache() if str(DEVICE) == 'cuda' else None  # Fixed device check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f569b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Independent evaluation (can run after kernel restart)\n",
    "from src.evaluate import evaluate_all_saved_models\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß™ Starting INDEPENDENT evaluation of all saved models...\")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate all saved models (no need for all_results in memory)\n",
    "summary_data = evaluate_all_saved_models(test_ds)\n",
    "\n",
    "print(\"\\n‚úÖ All saved models evaluated and summarized!\")\n",
    "print(f\"üìä Performance plot saved to: experiment_performance_comparison.png\")\n",
    "\n",
    "# Show best model details\n",
    "if summary_data:\n",
    "    best_exp = summary_data[0]\n",
    "    print(f\"\\nüèÜ Best model: {best_exp['experiment']}\")\n",
    "    print(f\"   Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "    print(f\"   Transform: {best_exp['transform']}\")\n",
    "    print(f\"   Run Folder: {best_exp['run_folder']}\")\n",
    "else:\n",
    "    print(\"‚ùå No models were successfully evaluated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9A: Evaluate specific experiments using your experiment_configs\n",
    "from src.evaluate import evaluate_from_experiment_configs\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß™ Evaluating specific experiments from config...\")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate using your experiment_configs (finds latest runs automatically)\n",
    "summary_data = evaluate_from_experiment_configs(experiment_configs, test_ds)\n",
    "\n",
    "print(\"\\n‚úÖ Specific experiments evaluated!\")\n",
    "print(f\"üìä Performance plot saved to: experiment_performance_comparison.png\")\n",
    "\n",
    "# Show best model details\n",
    "if summary_data:\n",
    "    best_exp = summary_data[0]\n",
    "    print(f\"\\nüèÜ Best model: {best_exp['experiment']}\")\n",
    "    print(f\"   Run: {best_exp['run_name']}\")\n",
    "    print(f\"   Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "    print(f\"   Transform: {best_exp['transform']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbae024",
   "metadata": {},
   "source": [
    "---\n",
    "##  Test Predictions\n",
    "Let's visualize some predictions from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc32f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize random predictions from test set\n",
    "def predict_and_visualize(dataset, index, model, processor):\n",
    "    \"\"\"Get an image from the dataset, run model prediction, and display results.\"\"\"\n",
    "    \n",
    "    img, true_label = dataset[index]\n",
    "    img_pil = transforms.ToPILImage()(img)\n",
    "    \n",
    "    # Run model\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    inputs = processor(images=img_pil, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Post-process\n",
    "    probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "    pred_label = torch.argmax(probs).item()\n",
    "    confidence = probs[pred_label].item()\n",
    "     \n",
    "    # Visualize\n",
    "    print(f\"Predicted Label: {EMOTION_LABELS[pred_label]} (Confidence: {confidence:.2%})\")\n",
    "    print(f\"True Label:      {EMOTION_LABELS[true_label]}\")\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    top3_probs, top3_idx = torch.topk(probs, 3)\n",
    "    print(\"\\nTop 3 Predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top3_probs, top3_idx)):\n",
    "        print(f\"  {i+1}. {EMOTION_LABELS[idx]}: {prob:.2%}\")\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(img_pil, cmap='gray')\n",
    "    plt.title(f\"Predicted: {EMOTION_LABELS[pred_label]}\\nTrue: {EMOTION_LABELS[true_label]}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return true_label, pred_label, confidence\n",
    "\n",
    "\n",
    "print(\"Testing predictions AFTER training:\\n\")\n",
    "\n",
    "num_samples = 5\n",
    "for i in range(num_samples):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Sample {i+1}/{num_samples}\")\n",
    "    print('='*70)\n",
    "    idx = random.randint(0, len(test_ds)-1)\n",
    "    true, pred, conf = predict_and_visualize(test_ds, idx, model, processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462663a",
   "metadata": {},
   "source": [
    "## Debug \n",
    "### Reload Failed Models from Backup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Training from Last Backup\n",
    "from src.backup import resume_training\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "MODELS_TO_RESUME = [\n",
    "    \"baseline_heavy\",\n",
    "]\n",
    "\n",
    "for model_folder in MODELS_TO_RESUME:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Resuming: {model_folder}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        run_folder = CHECKPOINTS_DIR / model_folder\n",
    "        \n",
    "        # Load training parameters to get original settings\n",
    "        params_path = run_folder / \"training_parameters.json\"\n",
    "        with open(params_path, 'r') as f:\n",
    "            training_params = json.load(f)\n",
    "        \n",
    "        # Create fresh model and datasets\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\",\n",
    "            num_labels=7,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Determine transform\n",
    "        transform_key = \"none\"\n",
    "        if 'heavy' in model_folder.lower():\n",
    "            transform_key = \"heavy\"\n",
    "        elif 'medium' in model_folder.lower():\n",
    "            transform_key = \"medium\"\n",
    "        elif 'light' in model_folder.lower():\n",
    "            transform_key = \"light\"\n",
    "        \n",
    "        transform = transform_configs[transform_key]\n",
    "        \n",
    "        train_ds = FER2013Dataset(split=\"train\", transform=transform)\n",
    "        val_ds = FER2013Dataset(split=\"valid\", transform=base_transform())\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            model.parameters(), \n",
    "            lr=training_params['learning_rate'],\n",
    "            weight_decay=training_params['optimizer_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Resume training\n",
    "        model_resumed, history, new_run_folder = resume_training(\n",
    "            run_folder=run_folder,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train_ds,\n",
    "            val_dataset=val_ds,\n",
    "            num_epochs=training_params['num_epochs'], \n",
    "            batch_size=training_params['batch_size'],\n",
    "            device=\"cuda\",\n",
    "            model_name=f\"resumed_{model_folder}\",\n",
    "            use_wandb=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully resumed: {model_folder}\")\n",
    "        print(f\"New run folder: {new_run_folder}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to resume {model_folder}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion Classifier ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
