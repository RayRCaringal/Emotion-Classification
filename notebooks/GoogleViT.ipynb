{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6402fff3",
   "metadata": {},
   "source": [
    "# Google ViT \n",
    "## Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6d4a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 \n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from transformers import ViTForImageClassification\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from src.transforms import base_transform\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.config import (\n",
    "    DEVICE, \n",
    "    NUM_LABELS, \n",
    "    EMOTION_LABELS,\n",
    "    DEFAULT_BATCH_SIZE,\n",
    "    DEFAULT_LEARNING_RATE\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from src.train import train_model\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91481961",
   "metadata": {},
   "source": [
    "### Weights and Biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f21c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Ray\\_netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Weights & Biases...\n",
      "WandB mode set to: ONLINE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mraycaringal\u001b[0m (\u001b[33mraycaringal-university-of-texas-austin\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized in ONLINE mode for project: emotion-classifier-vit\n",
      "Current WandB mode: ONLINE\n",
      "W&B initialized successfully in ONLINE mode!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 \n",
    "from src.wandb_utils import login, check_wandb_mode, sync_offline_runs\n",
    "\n",
    "# \"online\", \"offline\", or \"disabled\"\n",
    "# If set to offlien dont forget to sink\n",
    "WANDB_MODE = \"online\" \n",
    "\n",
    "print(\"Initializing Weights & Biases...\")\n",
    "current_mode = login(\n",
    "    project=\"emotion-classifier-vit\",\n",
    "    mode=WANDB_MODE\n",
    ")\n",
    "\n",
    "print(f\"W&B initialized successfully in {current_mode.upper()} mode!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac93a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.wandb_utils import *\n",
    "\n",
    "# Weights and Biases Util Commands \n",
    "\n",
    "# Check current mode\n",
    "# check_wandb_mode()\n",
    "\n",
    "# Sync offline runs (when you have internet)\n",
    "# sync_offline_runs(all_runs=True)\n",
    "\n",
    "# List available offline runs\n",
    "# list_offline_runs()\n",
    "\n",
    "# Change mode \n",
    "# set_wandb_mode(\"offline\")  \n",
    "\n",
    "# Set Confirm to False for a Dry Run\n",
    "# clear_offline_runs(confirm=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebee14",
   "metadata": {},
   "source": [
    "### Tranformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b7bbce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Configs Loaded\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Simpler transformation sets without deprecated parameters\n",
    "transform_configs = {\n",
    "    \"none\": base_transform(),  # Use the base transforms from transforms.py\n",
    "    \n",
    "    \"light\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        A.Affine(translate_percent=0.05, scale=(0.95, 1.05), rotate=(-10, 10), p=0.3),\n",
    "        *base_transform()  \n",
    "    ]),\n",
    "    \n",
    "    \"medium\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Affine(translate_percent=0.1, scale=(0.9, 1.1), rotate=(-15, 15), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        *base_transform() \n",
    "    ]),\n",
    "    \n",
    "    \"heavy\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Affine(translate_percent=0.15, scale=(0.85, 1.15), rotate=(-20, 20), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
    "        A.GridDropout(ratio=0.1, p=0.3),\n",
    "        *base_transform()  \n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Transformation Configs Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9719f4",
   "metadata": {},
   "source": [
    "---\n",
    "##  Fine Tuning Section\n",
    "Using FER2013 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6141",
   "metadata": {},
   "source": [
    "### Hyper Parameter Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3192cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Experiment Configs Loaded\n"
     ]
    }
   ],
   "source": [
    "# Define experiment configurations\n",
    "EPOCHS = 7\n",
    "\n",
    "experiment_configs = [\n",
    "    # Baseline with different transforms\n",
    "    {\n",
    "        \"name\": \"baseline_none_linear_probe\",\n",
    "        \"transform_key\": \"none\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"baseline_light_long\",\n",
    "    #     \"transform_key\": \"light\", \n",
    "    #     \"epochs\": EPOCHS,\n",
    "    #     \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "    #     \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "    #     \"weight_decay\": 0.01\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"baseline_medium_linear_probe\",\n",
    "        \"transform_key\": \"medium\",\n",
    "        \"epochs\": EPOCHS, \n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_heavy_linear_probe\",\n",
    "        \"transform_key\": \"heavy\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE, \n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"{len(experiment_configs)} Experiment Configs Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b0475",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from src.wandb_utils import cleanup_wandb_run\n",
    "\n",
    "all_results = {}\n",
    "failed_experiments = []\n",
    "\n",
    "print(f\"Starting training for {len(experiment_configs)} experiments\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, config in enumerate(tqdm(experiment_configs, desc=\"Training Experiments\")):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî¨ Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    print(f\"   Transform: {config['transform_key']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch Size: {config['batch_size']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Ensure any previous WandB run is cleaned up\n",
    "    cleanup_wandb_run()\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        transform = transform_configs[config['transform_key']]\n",
    "        \n",
    "        train = FER2013Dataset(\n",
    "            split=\"train\",\n",
    "            transform=transform\n",
    "        )\n",
    "        valid = FER2013Dataset(\n",
    "            split=\"valid\", \n",
    "            transform=base_transform()\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "        \n",
    "        # Train model\n",
    "        model_exp, history_exp, run_folder_exp = train_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train,\n",
    "            val_dataset=valid,\n",
    "            num_epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            device=DEVICE,\n",
    "            model_name=config['name'],  \n",
    "            use_wandb=True,\n",
    "            wandb_config={\n",
    "                \"learning_rate\": config['learning_rate'],\n",
    "                \"batch_size\": config['batch_size'],\n",
    "                \"epochs\": config['epochs'],\n",
    "                \"weight_decay\": config['weight_decay'],\n",
    "                \"model_name\": \"vit_base_patch16_224\",\n",
    "                \"architecture\": \"ViT\", \n",
    "                \"dataset\": \"FER2013\",\n",
    "                \"transform_set\": config['transform_key'],\n",
    "                \"experiment_name\": config['name']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[config['name']] = {\n",
    "            'model': model_exp,\n",
    "            'history': history_exp,\n",
    "            'run_folder': run_folder_exp,\n",
    "            'config': config,\n",
    "            'best_val_accuracy': max(history_exp['val_acc']),      \n",
    "            'best_val_loss': min(history_exp['val_loss']),\n",
    "            'final_train_accuracy': history_exp['train_acc'][-1],  \n",
    "            'final_train_loss': history_exp['train_loss'][-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n COMPLETED: {config['name']}\")\n",
    "        print(f\"   Best Val Accuracy: {all_results[config['name']]['best_val_accuracy']:.4f}\")\n",
    "        print(f\"   Best Val Loss: {all_results[config['name']]['best_val_loss']:.4f}\")\n",
    "        print(f\"   Run folder: {run_folder_exp}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n  Training interrupted by user at experiment: {config['name']}\")\n",
    "        cleanup_wandb_run()\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR in experiment {config['name']}: {str(e)}\")\n",
    "        print(f\"   Exception type: {type(e).__name__}\")\n",
    "        \n",
    "        # Store failed experiment info\n",
    "        failed_experiments.append({\n",
    "            'name': config['name'],\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__\n",
    "        })\n",
    "        \n",
    "        # Clean up WandB\n",
    "        cleanup_wandb_run()\n",
    "        \n",
    "        # Decide whether to continue or stop\n",
    "        print(f\"   Continuing to next experiment...\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up memory regardless of success/failure\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'model_exp' in locals():\n",
    "            del model_exp\n",
    "        if 'optimizer' in locals():\n",
    "            del optimizer\n",
    "        if 'train' in locals():\n",
    "            del train\n",
    "        if 'valid' in locals():\n",
    "            del valid\n",
    "            \n",
    "        # Force garbage collection\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        print(f\"   Memory cleaned up\")\n",
    "\n",
    "# Final cleanup\n",
    "cleanup_wandb_run()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\" Successful experiments: {len(all_results)}/{len(experiment_configs)}\")\n",
    "print(f\" Failed experiments: {len(failed_experiments)}/{len(experiment_configs)}\")\n",
    "\n",
    "if all_results:\n",
    "    print(\"\\n Results:\")\n",
    "    for name, result in all_results.items():\n",
    "        print(f\"   {name}: Val Acc = {result['best_val_accuracy']:.4f}, Val Loss = {result['best_val_loss']:.4f}\")\n",
    "\n",
    "if failed_experiments:\n",
    "    print(\"\\n  Failed Experiments:\")\n",
    "    for failed in failed_experiments:\n",
    "        print(f\"   {failed['name']}: {failed['error_type']} - {failed['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5a25c",
   "metadata": {},
   "source": [
    "### Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a4803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LINEAR PROBE training for 1 experiments\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88291e3994cd4843b9862ed8bd6114d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Experiments:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üî¨ Experiment 1/1: baseline_light_long\n",
      "   Training Type: LINEAR PROBE (frozen encoder)\n",
      "   Transform: light\n",
      "   LR: 2e-05\n",
      "   Epochs: 20\n",
      "   Batch Size: 32\n",
      "======================================================================\n",
      "WandB run cleaned up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model initialized\n",
      "   Total parameters: 85,804,039\n",
      "   Classifier parameters: 5,383\n",
      "Freezing encoder parameters...\n",
      "Trainable parameters: 5,383 / 85,804,039\n",
      "Percentage trainable: 0.01%\n",
      "Created run folder: baseline_light_long1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\notebooks\\wandb\\run-20251207_131628-qatag4qj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj' target=\"_blank\">baseline_light_long1</a></strong> to <a href='https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification' target=\"_blank\">https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj' target=\"_blank\">https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run started: baseline_light_long1\n",
      "WandB Dashboard: https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj\n",
      "Training parameters saved to: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\training_parameters.json\n",
      "Training baseline_light_long for 20 epochs...\n",
      "Training type: LINEAR PROBE (frozen encoder)\n",
      "Run name (WandB): baseline_light_long1\n",
      "Total training steps: 17960\n",
      "Device: cuda\n",
      "Batch size: 32\n",
      "Train batches: 898\n",
      "Val batches: 113\n",
      "Run folder: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\n",
      "Best model will be saved to: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "Backup interval: every 5 epochs\n",
      "W&B tracking: https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj\n",
      "======================================================================\n",
      "\n",
      "Epoch 1/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ed276303ce4214be89b14b359d733d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9d6f3ffada422a8e44f4a6180d6ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 0:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.8614 | Train Acc: 0.2797 | Train F1: 0.1981\n",
      "Val Loss:   1.7752 | Val Acc:   0.3070 | Val F1:   0.2053\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.3070, Val F1: 0.2053)\n",
      "Backup created: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\backups\\backup_epoch_000_20251207_131815.pth\n",
      "\n",
      "Epoch 2/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e88c95cf49467d9b5c4fc9f0d12b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a48089eb6e14d6fb6b951285246b981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 1:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.7199 | Train Acc: 0.3392 | Train F1: 0.2563\n",
      "Val Loss:   1.6695 | Val Acc:   0.3778 | Val F1:   0.3052\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.3778, Val F1: 0.3052)\n",
      "\n",
      "Epoch 3/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f33d2c530c74f0fb080853923ae4750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a91e250cd284a7b8912151512a6bd6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 2:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.6378 | Train Acc: 0.3953 | Train F1: 0.3362\n",
      "Val Loss:   1.6029 | Val Acc:   0.4157 | Val F1:   0.3566\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4157, Val F1: 0.3566)\n",
      "\n",
      "Epoch 4/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1dacc67ff8401f9c33814514416ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47254449d1f14cb9b3154ff762dda07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 3:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.5839 | Train Acc: 0.4260 | Train F1: 0.3772\n",
      "Val Loss:   1.5557 | Val Acc:   0.4416 | Val F1:   0.3918\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4416, Val F1: 0.3918)\n",
      "\n",
      "Epoch 5/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491c85fd268f42858fcec87b2a3da9a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037dfc0c51d04e06a4b8626e4f405067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 4:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.5434 | Train Acc: 0.4456 | Train F1: 0.4034\n",
      "Val Loss:   1.5208 | Val Acc:   0.4503 | Val F1:   0.4058\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4503, Val F1: 0.4058)\n",
      "\n",
      "Epoch 6/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c895ea3529854791872dc0ec6577f11d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b76ff15c9b429aadd302548a2899de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 5:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.5123 | Train Acc: 0.4556 | Train F1: 0.4171\n",
      "Val Loss:   1.4937 | Val Acc:   0.4606 | Val F1:   0.4198\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4606, Val F1: 0.4198)\n",
      "Backup created: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\backups\\backup_epoch_005_20251207_132645.pth\n",
      "\n",
      "Epoch 7/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4529467b2e435691a06c2d6a624bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd0db6f7a13421abb51e214501bb23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 6:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4886 | Train Acc: 0.4679 | Train F1: 0.4330\n",
      "Val Loss:   1.4724 | Val Acc:   0.4664 | Val F1:   0.4279\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4664, Val F1: 0.4279)\n",
      "\n",
      "Epoch 8/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1b23c2abd74b2c81f3ebe4fdc74ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff94b47cd5d4db18fc15c53fdbf3269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 7:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4708 | Train Acc: 0.4713 | Train F1: 0.4385\n",
      "Val Loss:   1.4555 | Val Acc:   0.4714 | Val F1:   0.4351\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4714, Val F1: 0.4351)\n",
      "\n",
      "Epoch 9/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee75032bacb4802b0d50ed057359d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328169e755294f69ba9d9962a4e2f12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 8:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4563 | Train Acc: 0.4765 | Train F1: 0.4451\n",
      "Val Loss:   1.4416 | Val Acc:   0.4748 | Val F1:   0.4394\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4748, Val F1: 0.4394)\n",
      "\n",
      "Epoch 10/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce567a0b70bb4d1596b4c7a377395967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c12657d4124ede894db82630ba6ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 9:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4438 | Train Acc: 0.4764 | Train F1: 0.4457\n",
      "Val Loss:   1.4302 | Val Acc:   0.4784 | Val F1:   0.4446\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4784, Val F1: 0.4446)\n",
      "\n",
      "Epoch 11/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbcc4f6b35f4b1c89704e825f290d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4111213d571c4beb94e6fa3376bc284e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 10:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4322 | Train Acc: 0.4808 | Train F1: 0.4517\n",
      "Val Loss:   1.4207 | Val Acc:   0.4801 | Val F1:   0.4470\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4801, Val F1: 0.4470)\n",
      "Backup created: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\backups\\backup_epoch_010_20251207_133546.pth\n",
      "\n",
      "Epoch 12/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f4d9f8e86a4c1ca4538f238c24c403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 11:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5d2b012b1e4a8a8a4dd5e32452b805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 11:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4236 | Train Acc: 0.4825 | Train F1: 0.4540\n",
      "Val Loss:   1.4130 | Val Acc:   0.4801 | Val F1:   0.4476\n",
      "\n",
      "Epoch 13/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00ac020f966423fa1593e7d10f2f82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 12:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58df3c6b0b8347d0ad3e8edec4028cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 12:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4152 | Train Acc: 0.4854 | Train F1: 0.4568\n",
      "Val Loss:   1.4067 | Val Acc:   0.4806 | Val F1:   0.4490\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4806, Val F1: 0.4490)\n",
      "\n",
      "Epoch 14/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ae61e2c9a54c4c972dce4a89941431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 13:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1388169827b14141928a6ef2f9086d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 13:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4107 | Train Acc: 0.4871 | Train F1: 0.4595\n",
      "Val Loss:   1.4013 | Val Acc:   0.4837 | Val F1:   0.4525\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4837, Val F1: 0.4525)\n",
      "\n",
      "Epoch 15/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552b83043edb49088566eeeb66501e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 14:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50f94181bbf4777be084487ceb2e1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 14:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4048 | Train Acc: 0.4891 | Train F1: 0.4614\n",
      "Val Loss:   1.3971 | Val Acc:   0.4834 | Val F1:   0.4528\n",
      "\n",
      "Epoch 16/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb7090f7bd1477685680abc14354d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 15:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b73d8a6200480f9ff639452ac93f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 15:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.4041 | Train Acc: 0.4886 | Train F1: 0.4616\n",
      "Val Loss:   1.3937 | Val Acc:   0.4840 | Val F1:   0.4537\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4840, Val F1: 0.4537)\n",
      "Backup created: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\backups\\backup_epoch_015_20251207_134408.pth\n",
      "\n",
      "Epoch 17/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8326d07b3b1f4f258e7b30d8e87a09ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 16:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86532812c57a44f1900f43c10cb4a8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 16:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.3992 | Train Acc: 0.4906 | Train F1: 0.4637\n",
      "Val Loss:   1.3912 | Val Acc:   0.4840 | Val F1:   0.4539\n",
      "\n",
      "Epoch 18/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d295fd73dc7f401290bebdce0bbc667f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 17:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a541b2ab164c239b376a9ffe62a053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 17:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.3970 | Train Acc: 0.4905 | Train F1: 0.4638\n",
      "Val Loss:   1.3895 | Val Acc:   0.4845 | Val F1:   0.4549\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4845, Val F1: 0.4549)\n",
      "\n",
      "Epoch 19/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c39bc84e5504f1187ea3437a8ced583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 18:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b12a87f4184a3fac8dbfc0ec228f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 18:   0%|          | 0/113 [00:40<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.3955 | Train Acc: 0.4912 | Train F1: 0.4646\n",
      "Val Loss:   1.3884 | Val Acc:   0.4845 | Val F1:   0.4550\n",
      "\n",
      "Epoch 20/20\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfafce9cc25446dbe7d885dc2c90213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 19:   0%|          | 0/898 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a870351a311d45909280c89683a3f83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating Epoch 19:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 1.3946 | Train Acc: 0.4911 | Train F1: 0.4648\n",
      "Val Loss:   1.3881 | Val Acc:   0.4848 | Val F1:   0.4553\n",
      "Checkpoint saved: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\best_baseline_light_long.pth\n",
      "‚úÖ New best model saved! (Val Acc: 0.4848, Val F1: 0.4553)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Training completed!\n",
      "Best validation accuracy: 0.4848\n",
      "Training parameters saved to: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\training_parameters.json\n",
      "Training history saved to: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\history_baseline_light_long1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `quiet` argument to `wandb.run.finish()` is deprecated, use `wandb.Settings(quiet=...)` to set this instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final backup created: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\backups\\backup_final_20251207_135136.pth\n",
      "Training completed successfully - cleaning up all backups...\n",
      "üßπ Deleted backup: backup_epoch_000_20251207_131815.pth\n",
      "üßπ Deleted backup: backup_epoch_005_20251207_132645.pth\n",
      "üßπ Deleted backup: backup_epoch_010_20251207_133546.pth\n",
      "üßπ Deleted backup: backup_epoch_015_20251207_134408.pth\n",
      "üßπ Deleted backup: backup_final_20251207_135136.pth\n",
      "Deleted all 5 backup files after successful training\n",
      "Deleted 5 backup files\n",
      "  Removed empty backups directory: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\\backups\n",
      "Backups folder successfully removed\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñÅ‚ñà‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÖ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñà‚ñÅ‚ñÖ‚ñÜ</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/batch_loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/accuracy</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch</td><td>800</td></tr><tr><td>epoch</td><td>19</td></tr><tr><td>train/batch_loss</td><td>1.46164</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.39462</td></tr><tr><td>val/accuracy</td><td>0.48481</td></tr><tr><td>val/f1</td><td>0.45527</td></tr><tr><td>val/loss</td><td>1.3881</td></tr><tr><td>val/precision</td><td>0.4504</td></tr><tr><td>val/recall</td><td>0.48481</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">baseline_light_long1</strong> at: <a href='https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj' target=\"_blank\">https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification/runs/qatag4qj</a><br> View project at: <a href='https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification' target=\"_blank\">https://wandb.ai/raycaringal-university-of-texas-austin/emotion-classification</a><br>Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251207_131628-qatag4qj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB run cleaned up\n",
      "\n",
      "‚úÖ COMPLETED: baseline_light_long\n",
      "   Best Val Accuracy: 0.4848\n",
      "   Best Val Loss: 1.3881\n",
      "   Run folder: c:\\Users\\Ray\\Documents\\AI\\Emotion-Classification\\checkpoints\\baseline_light_long1\n",
      "   Memory cleaned up\n",
      "WandB run cleaned up\n",
      "\n",
      "======================================================================\n",
      "üìä LINEAR PROBE TRAINING COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "‚úÖ Successful experiments: 1/1\n",
      "‚ùå Failed experiments: 0/1\n",
      "\n",
      "üìà Results:\n",
      "   baseline_light_long: Val Acc = 0.4848, Val Loss = 1.3881\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from src.wandb_utils import cleanup_wandb_run\n",
    "from src.linear_probe import train_linear_probe\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "all_results = {}\n",
    "failed_experiments = []\n",
    "\n",
    "print(f\"Starting LINEAR PROBE training for {len(experiment_configs)} experiments\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, config in enumerate(tqdm(experiment_configs, desc=\"Training Experiments\")):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî¨ Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    print(f\"   Training Type: LINEAR PROBE (frozen encoder)\")\n",
    "    print(f\"   Transform: {config['transform_key']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch Size: {config['batch_size']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Ensure any previous WandB run is cleaned up\n",
    "    cleanup_wandb_run()\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        transform = transform_configs[config['transform_key']]\n",
    "        \n",
    "        train = FER2013Dataset(\n",
    "            split=\"train\",\n",
    "            transform=transform\n",
    "        )\n",
    "        valid = FER2013Dataset(\n",
    "            split=\"valid\", \n",
    "            transform=base_transform()\n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # IMPORTANT: For linear probe, only optimize classifier parameters\n",
    "        # The encoder will be frozen inside train_linear_probe()\n",
    "        optimizer = AdamW(\n",
    "            model.classifier.parameters(),  # Only classifier, not model.parameters()\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model initialized\")\n",
    "        print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        print(f\"   Classifier parameters: {sum(p.numel() for p in model.classifier.parameters()):,}\")\n",
    "        \n",
    "        # Train model with LINEAR PROBE\n",
    "        model_exp, history_exp, run_folder_exp = train_linear_probe(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train,\n",
    "            val_dataset=valid,\n",
    "            num_epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            device=DEVICE,\n",
    "            model_name=config['name'],  \n",
    "            use_wandb=True,\n",
    "            wandb_config={\n",
    "                \"learning_rate\": config['learning_rate'],\n",
    "                \"batch_size\": config['batch_size'],\n",
    "                \"epochs\": config['epochs'],\n",
    "                \"weight_decay\": config['weight_decay'],\n",
    "                \"model_name\": \"vit_base_patch16_224\",\n",
    "                \"architecture\": \"ViT\", \n",
    "                \"dataset\": \"FER2013\",\n",
    "                \"transform_set\": config['transform_key'],\n",
    "                \"experiment_name\": config['name'],\n",
    "                \"training_type\": \"linear_probe\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[config['name']] = {\n",
    "            'model': model_exp,\n",
    "            'history': history_exp,\n",
    "            'run_folder': run_folder_exp,\n",
    "            'config': config,\n",
    "            'best_val_accuracy': max(history_exp['val_acc']),      \n",
    "            'best_val_loss': min(history_exp['val_loss']),\n",
    "            'final_train_accuracy': history_exp['train_acc'][-1],  \n",
    "            'final_train_loss': history_exp['train_loss'][-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ COMPLETED: {config['name']}\")\n",
    "        print(f\"   Best Val Accuracy: {all_results[config['name']]['best_val_accuracy']:.4f}\")\n",
    "        print(f\"   Best Val Loss: {all_results[config['name']]['best_val_loss']:.4f}\")\n",
    "        print(f\"   Run folder: {run_folder_exp}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è  Training interrupted by user at experiment: {config['name']}\")\n",
    "        cleanup_wandb_run()\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR in experiment {config['name']}: {str(e)}\")\n",
    "        print(f\"   Exception type: {type(e).__name__}\")\n",
    "        \n",
    "        # Store failed experiment info\n",
    "        failed_experiments.append({\n",
    "            'name': config['name'],\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__\n",
    "        })\n",
    "        \n",
    "        # Clean up WandB\n",
    "        cleanup_wandb_run()\n",
    "        \n",
    "        # Decide whether to continue or stop\n",
    "        print(f\"   Continuing to next experiment...\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up memory regardless of success/failure\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'model_exp' in locals():\n",
    "            del model_exp\n",
    "        if 'optimizer' in locals():\n",
    "            del optimizer\n",
    "        if 'train' in locals():\n",
    "            del train\n",
    "        if 'valid' in locals():\n",
    "            del valid\n",
    "            \n",
    "        # Force garbage collection\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        print(f\"   Memory cleaned up\")\n",
    "\n",
    "# Final cleanup\n",
    "cleanup_wandb_run()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìä LINEAR PROBE TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"‚úÖ Successful experiments: {len(all_results)}/{len(experiment_configs)}\")\n",
    "print(f\"‚ùå Failed experiments: {len(failed_experiments)}/{len(experiment_configs)}\")\n",
    "\n",
    "if all_results:\n",
    "    print(\"\\nüìà Results:\")\n",
    "    for name, result in all_results.items():\n",
    "        print(f\"   {name}: Val Acc = {result['best_val_accuracy']:.4f}, Val Loss = {result['best_val_loss']:.4f}\")\n",
    "\n",
    "if failed_experiments:\n",
    "    print(\"\\n‚ö†Ô∏è  Failed Experiments:\")\n",
    "    for failed in failed_experiments:\n",
    "        print(f\"   {failed['name']}: {failed['error_type']} - {failed['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f569b0",
   "metadata": {},
   "source": [
    "---\n",
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Independent evaluation (can run after kernel restart)\n",
    "from src.evaluate import evaluate_all_saved_models\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß™ Starting INDEPENDENT evaluation of all saved models...\")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate all saved models (no need for all_results in memory)\n",
    "summary_data = evaluate_all_saved_models(test_ds)\n",
    "\n",
    "print(\"\\n‚úÖ All saved models evaluated and summarized!\")\n",
    "print(f\"üìä Performance plot saved to: experiment_performance_comparison.png\")\n",
    "\n",
    "# Show best model details\n",
    "if summary_data:\n",
    "    best_exp = summary_data[0]\n",
    "    print(f\"\\nüèÜ Best model: {best_exp['experiment']}\")\n",
    "    print(f\"   Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "    print(f\"   Transform: {best_exp['transform']}\")\n",
    "    print(f\"   Run Folder: {best_exp['run_folder']}\")\n",
    "else:\n",
    "    print(\"‚ùå No models were successfully evaluated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9A: Evaluate specific experiments using your experiment_configs\n",
    "from src.evaluate import evaluate_from_experiment_configs\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß™ Evaluating specific experiments from config...\")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate using your experiment_configs (finds latest runs automatically)\n",
    "summary_data = evaluate_from_experiment_configs(experiment_configs, test_ds)\n",
    "\n",
    "print(\"\\n‚úÖ Specific experiments evaluated!\")\n",
    "print(f\"üìä Performance plot saved to: experiment_performance_comparison.png\")\n",
    "\n",
    "# Show best model details\n",
    "if summary_data:\n",
    "    best_exp = summary_data[0]\n",
    "    print(f\"\\nüèÜ Best model: {best_exp['experiment']}\")\n",
    "    print(f\"   Run: {best_exp['run_name']}\")\n",
    "    print(f\"   Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "    print(f\"   Transform: {best_exp['transform']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbae024",
   "metadata": {},
   "source": [
    "---\n",
    "###  Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc32f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions from multiple models\n",
    "from src.metadata import find_latest_run_for_experiment, load_training_parameters\n",
    "from src.checkpoint_utils import load_model_from_checkpoint\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"baseline_none\",\n",
    "    \"baseline_light\", \n",
    "]\n",
    "\n",
    "NUM_SAMPLES = 3  # Number of random test samples per model\n",
    "\n",
    "def predict_and_visualize_batch(dataset, indices, model, processor, model_name):\n",
    "    \"\"\"Run predictions on multiple samples for a single model.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img, true_label = dataset[idx]\n",
    "        img_pil = transforms.ToPILImage()(img)\n",
    "        \n",
    "        # Run model\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        inputs = processor(images=img_pil, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Post-process\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "        pred_label = torch.argmax(probs).item()\n",
    "        confidence = probs[pred_label].item()\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top3_probs, top3_idx = torch.topk(probs, 3)\n",
    "        top3_predictions = [\n",
    "            (EMOTION_LABELS[idx.item()], prob.item()) \n",
    "            for prob, idx in zip(top3_probs, top3_idx)\n",
    "        ]\n",
    "        \n",
    "        results.append({\n",
    "            'sample_index': idx,\n",
    "            'true_label': true_label,\n",
    "            'pred_label': pred_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == pred_label,\n",
    "            'top3_predictions': top3_predictions,\n",
    "            'image': img_pil\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_model_predictions(model_name, results, sample_indices):\n",
    "    \"\"\"Display predictions for a single model.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    correct_count = sum(1 for r in results if r['correct'])\n",
    "    accuracy = correct_count / len(results)\n",
    "    \n",
    "    print(f\"Batch Accuracy: {correct_count}/{len(results)} ({accuracy:.1%})\")\n",
    "    print(f\"Samples tested: {sample_indices}\")\n",
    "    print()\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Sample {i+1} (Index {result['sample_index']}):\")\n",
    "        print(f\"  True: {EMOTION_LABELS[result['true_label']]:<12}\", end=\"\")\n",
    "        print(f\"  Predicted: {EMOTION_LABELS[result['pred_label']]:<12}\", end=\"\")\n",
    "        print(f\"  Confidence: {result['confidence']:.1%}\", end=\"\")\n",
    "        print(f\"  {'‚úì' if result['correct'] else '‚úó'}\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        print(f\"  Top 3: \", end=\"\")\n",
    "        for j, (emotion, prob) in enumerate(result['top3_predictions']):\n",
    "            print(f\"{emotion}: {prob:.1%}\", end=\"\")\n",
    "            if j < 2:\n",
    "                print(\", \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize all samples in a grid\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(4*len(results), 4))\n",
    "    if len(results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (result, ax) in enumerate(zip(results, axes)):\n",
    "        ax.imshow(result['image'], cmap='gray')\n",
    "        correct = result['correct']\n",
    "        color = 'green' if correct else 'red'\n",
    "        title = f\"Sample {i+1}\\n\"\n",
    "        title += f\"True: {EMOTION_LABELS[result['true_label']]}\\n\"\n",
    "        title += f\"Pred: {EMOTION_LABELS[result['pred_label']]}\\n\"\n",
    "        title += f\"Conf: {result['confidence']:.1%}\"\n",
    "        ax.set_title(title, color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Model: {model_name} (Accuracy: {accuracy:.1%})\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Load processor (same for all models)\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} models on {NUM_SAMPLES} random samples each\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "print()\n",
    "\n",
    "# Get random sample indices (same for all models for fair comparison)\n",
    "sample_indices = random.sample(range(len(test_ds)), NUM_SAMPLES)\n",
    "print(f\"Random sample indices: {sample_indices}\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    try:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Find the latest run for this model\n",
    "        run_folder = find_latest_run_for_experiment(model_name, CHECKPOINTS_DIR)\n",
    "        \n",
    "        # Load the best model checkpoint\n",
    "        checkpoint_path = run_folder / f\"best_{run_folder.name}.pth\"\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Load model\n",
    "        model = load_model_from_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Get model info\n",
    "        params = load_training_parameters(run_folder)\n",
    "        print(f\"Loaded: {run_folder.name}\")\n",
    "        print(f\"Transform: {model_name.split('_')[-1]}\")\n",
    "        print(f\"Epochs: {params.get('num_epochs', 'N/A')}\")\n",
    "        print(f\"Learning rate: {params.get('learning_rate', 'N/A'):.2e}\")\n",
    "        \n",
    "        # Run predictions\n",
    "        results = predict_and_visualize_batch(\n",
    "            dataset=test_ds,\n",
    "            indices=sample_indices,\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        accuracy = display_model_predictions(model_name, results, sample_indices)\n",
    "        model_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': sum(1 for r in results if r['correct']),\n",
    "            'total': len(results),\n",
    "            'run_folder': run_folder.name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to test {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Print summary comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY: Model Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if model_results:\n",
    "    # Sort by accuracy\n",
    "    sorted_results = sorted(\n",
    "        model_results.items(), \n",
    "        key=lambda x: x[1]['accuracy'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerformance Ranking:\")\n",
    "    for i, (model_name, result) in enumerate(sorted_results):\n",
    "        print(f\"{i+1}. {model_name:<20} {result['correct']}/{result['total']} ({result['accuracy']:.1%})\")\n",
    "    \n",
    "    # Best and worst performers\n",
    "    best_model = sorted_results[0]\n",
    "    worst_model = sorted_results[-1]\n",
    "    \n",
    "    print(f\"\\nBest: {best_model[0]} ({best_model[1]['accuracy']:.1%})\")\n",
    "    print(f\"Worst: {worst_model[0]} ({worst_model[1]['accuracy']:.1%})\")\n",
    "    \n",
    "    # Optional: Create comparison visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    models = [m[0] for m in sorted_results]\n",
    "    accuracies = [m[1]['accuracy'] for m in sorted_results]\n",
    "    \n",
    "    bars = ax.bar(models, accuracies, color=['green', 'lightgreen', 'orange', 'red'])\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Model Comparison on {NUM_SAMPLES} Samples')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.02,\n",
    "                f'{acc:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models were successfully tested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b1e44",
   "metadata": {},
   "source": [
    "## LoRa Section\n",
    "### Hyper Parameter Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ac4da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.lora import (\n",
    "    create_lora_config,\n",
    "    get_lora_model,\n",
    "    train_lora_model,\n",
    "    load_lora_model,\n",
    "    merge_and_save_lora_model,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA utilities imported successfully\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CELL: Define LoRA Experiment Configurations\n",
    "# ============================================================================\n",
    "# Define LoRA experiment configurations\n",
    "LORA_EPOCHS = 8\n",
    "\n",
    "lora_experiment_configs = [\n",
    "    {\n",
    "        \"name\": f\"lora_r4_dropout_{dropout}\",\n",
    "        \"transform_key\": \"light\",\n",
    "        \"epochs\": 8,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"lora_r\": 4,\n",
    "        \"lora_alpha\": 8,\n",
    "        \"lora_dropout\": dropout,\n",
    "        \"target_modules\": [\"query\", \"value\"],\n",
    "    }\n",
    "    for dropout in [0.0, 0.05, 0.1, 0.15, 0.2, 0.3, 0.5]  # Test wide range\n",
    "]\n",
    "\n",
    "# lora_experiment_configs = []\n",
    "# for r in [8, 16, 32]:\n",
    "#     for alpha_ratio in [0.5, 1, 2, 4]:  # alpha = r * ratio\n",
    "#         alpha = r * alpha_ratio\n",
    "#         lora_experiment_configs.append({\n",
    "#             \"name\": f\"lora_r{r}_alpha{alpha}\",\n",
    "#             \"transform_key\": \"light\",\n",
    "#             \"epochs\": 8,\n",
    "#             \"learning_rate\": 5e-4,\n",
    "#             \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#             \"weight_decay\": 0.01,\n",
    "#             \"lora_r\": r,\n",
    "#             \"lora_alpha\": alpha,\n",
    "#             \"lora_dropout\": 0.1,\n",
    "#             \"target_modules\": [\"query\", \"value\"],\n",
    "#         })\n",
    "\n",
    "\n",
    "# lora_experiment_configs = [\n",
    "#     # Fix r=4, vary alpha\n",
    "#     {\n",
    "#         \"name\": f\"lora_r4_alpha_{alpha}\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": 8,\n",
    "#         \"learning_rate\": 5e-4,  # Your best LR\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 4,\n",
    "#         \"lora_alpha\": alpha,\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     }\n",
    "#     for alpha in [2, 4, 8, 16, 32, 64]  # Test extreme values\n",
    "# ]\n",
    "\n",
    "# lora_experiment_configs = [\n",
    "#     # Base config with varying LRs\n",
    "#     {\n",
    "#         \"name\": f\"lora_r4_lr_{lr}\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": 8,\n",
    "#         \"learning_rate\": lr,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 4,\n",
    "#         \"lora_alpha\": 8,\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     }\n",
    "#     for lr in [1e-4, 3e-4, 5e-4, 8e-4, 1e-3, 2e-3, 3e-3]\n",
    "# ]\n",
    "\n",
    "\n",
    "# lora_experiment_configs = [\n",
    "#     {\n",
    "#         \"name\": \"lora_r32_light_short\",\n",
    "#         \"transform_key\": \"light\",  # Stick with what works\n",
    "#         \"epochs\": 8,  # Shorter training - you plateau early\n",
    "#         \"learning_rate\": 1e-3,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,  # Keep your original batch size\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 32,\n",
    "#         \"lora_alpha\": 64,\n",
    "#         \"lora_dropout\": 0.1,  # Lower dropout for facial features\n",
    "#         \"target_modules\": [\"query\", \"value\", \"output.dense\"],  # Start with these\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\": \"lora_r48_light_focused\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": 8,\n",
    "#         \"learning_rate\": 8e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 48,\n",
    "#         \"lora_alpha\": 96,\n",
    "#         \"lora_dropout\": 0.15,\n",
    "#         \"target_modules\": [\"query\", \"value\"],  # Just Q, V might be better\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# 2nd \n",
    "# lora_experiment_configs = [\n",
    "#     # Experiment 1: Baseline (your best performer)\n",
    "#     {\n",
    "#         \"name\": \"lora_r4_light_lr\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 5e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 4,\n",
    "#         \"lora_alpha\": 8,\n",
    "#         \"lora_dropout\": 0.05,  # Lower dropout for FER2013\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     },\n",
    "\n",
    "#     # Experiment 2: Query-only attention (simpler, fewer params)\n",
    "#     {\n",
    "#         \"name\": \"lora_r4_light_query_only\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 4,\n",
    "#         \"lora_alpha\": 8,\n",
    "#         \"lora_dropout\": 0.05,\n",
    "#         \"target_modules\": [\"query\"],  # Only query projections\n",
    "#     },\n",
    "#     # Experiment 3: Even lower dropout for subtle facial features\n",
    "#     {\n",
    "#         \"name\": \"lora_r4_light_low_dropout\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 4,\n",
    "#         \"lora_alpha\": 8,\n",
    "#         \"lora_dropout\": 0.01,  # Minimal dropout\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     },\n",
    "#     # Experiment 4: Slightly higher rank for facial detail\n",
    "#     {\n",
    "#         \"name\": \"lora_r6_light\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 3e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         \"lora_r\": 6,  # Slightly higher than r=4\n",
    "#         \"lora_alpha\": 12,  # 2 * r\n",
    "#         \"lora_dropout\": 0.05,\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# First \n",
    "# lora_experiment_configs = [\n",
    "#     # # Experiment 1: Conservative LoRA (low rank, efficient)\n",
    "#     {\n",
    "#         \"name\": \"lora_r4_light\",\n",
    "#         \"transform_key\": \"light\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 3e-4,  # Higher LR for LoRA\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         # LoRA specific parameters\n",
    "#         \"lora_r\": 4,\n",
    "#         \"lora_alpha\": 8,  # 2 * r\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     },\n",
    "#     # Experiment 2: Balanced LoRA (medium rank)\n",
    "#     {\n",
    "#         \"name\": \"lora_r8_medium\",\n",
    "#         \"transform_key\": \"medium\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 2e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         # LoRA specific parameters\n",
    "#         \"lora_r\": 8,\n",
    "#         \"lora_alpha\": 16,\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     },\n",
    "#     # Experiment 3: Higher capacity LoRA\n",
    "#     {\n",
    "#         \"name\": \"lora_r16_medium\",\n",
    "#         \"transform_key\": \"medium\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 1e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         # LoRA specific parameters\n",
    "#         \"lora_r\": 16,\n",
    "#         \"lora_alpha\": 32,\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"target_modules\": [\"query\", \"value\"],\n",
    "#     },\n",
    "#     # Experiment 4: Full attention LoRA (Q, K, V)\n",
    "#     {\n",
    "#         \"name\": \"lora_r8_qkv_heavy\",\n",
    "#         \"transform_key\": \"heavy\",\n",
    "#         \"epochs\": LORA_EPOCHS,\n",
    "#         \"learning_rate\": 2e-4,\n",
    "#         \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "#         \"weight_decay\": 0.01,\n",
    "#         # LoRA specific parameters\n",
    "#         \"lora_r\": 8,\n",
    "#         \"lora_alpha\": 16,\n",
    "#         \"lora_dropout\": 0.1,\n",
    "#         \"target_modules\": [\"query\", \"key\", \"value\"],\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "print(f\"{len(lora_experiment_configs)} LoRA Experiment Configs Loaded\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nüìä LoRA Configuration Summary:\")\n",
    "for config in lora_experiment_configs:\n",
    "    print(f\"\\n{config['name']}:\")\n",
    "    print(f\"  LoRA rank: {config['lora_r']}, alpha: {config['lora_alpha']}\")\n",
    "    print(f\"  Target modules: {config['target_modules']}\")\n",
    "    print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "    print(f\"  Transform: {config['transform_key']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5fbaf",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL: LoRA Training Loop (Fixed Version with Memory Management)\n",
    "# ============================================================================\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import gc\n",
    "from torch.optim import AdamW\n",
    "from src.wandb_utils import cleanup_wandb_run\n",
    "from peft import get_peft_model  # Direct import for debugging\n",
    "\n",
    "all_lora_results = {}\n",
    "failed_lora_experiments = []\n",
    "\n",
    "print(f\"Starting LoRA training for {len(lora_experiment_configs)} experiments\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, config in enumerate(tqdm(lora_experiment_configs, desc=\"LoRA Experiments\")):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LoRA Experiment {i+1}/{len(lora_experiment_configs)}: {config['name']}\")\n",
    "    print(f\"   Transform: {config['transform_key']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch Size: {config['batch_size']}\")\n",
    "    print(f\"   LoRA Rank: {config['lora_r']}, Alpha: {config['lora_alpha']}\")\n",
    "    print(f\"   Target Modules: {config['target_modules']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Ensure any previous WandB run is cleaned up\n",
    "    cleanup_wandb_run()\n",
    "    \n",
    "    try:\n",
    "        # Clear memory before starting new experiment\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            print(f\"GPU Memory cleared: {torch.cuda.memory_allocated() / 1e9:.2f} GB allocated\")\n",
    "        \n",
    "        # Create datasets\n",
    "        transform = transform_configs[config['transform_key']]\n",
    "        \n",
    "        train = FER2013Dataset(\n",
    "            split=\"train\",\n",
    "            transform=transform\n",
    "        )\n",
    "        valid = FER2013Dataset(\n",
    "            split=\"valid\", \n",
    "            transform=base_transform()\n",
    "        )\n",
    "        \n",
    "        # Initialize base model\n",
    "        print(\"Loading base model...\")\n",
    "        base_model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Create LoRA configuration\n",
    "        lora_config = create_lora_config(\n",
    "            r=config['lora_r'],\n",
    "            lora_alpha=config['lora_alpha'],\n",
    "            lora_dropout=config['lora_dropout'],\n",
    "            target_modules=config['target_modules'],\n",
    "        )\n",
    "        \n",
    "        print(f\"DEBUG: LoRA config created: r={lora_config.r}, target_modules={lora_config.target_modules}\")\n",
    "        \n",
    "        # Apply LoRA with classifier unfreezing\n",
    "        model = get_lora_model(base_model, lora_config, unfreeze_classifier=True)\n",
    "        \n",
    "        # Check trainable parameters BEFORE moving to device\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"DEBUG: Trainable parameters: {trainable_params:,} / {total_params:,} ({(trainable_params/total_params)*100:.2f}%)\")\n",
    "        \n",
    "        if trainable_params == 0:\n",
    "            print(\"‚ùå CRITICAL ERROR: No trainable parameters found!\")\n",
    "            print(\"DEBUG: Checking parameter names...\")\n",
    "            for name, param in model.named_parameters():\n",
    "                print(f\"  {name}: requires_grad={param.requires_grad}, shape={param.shape}\")\n",
    "            raise RuntimeError(\"No trainable parameters in model. LoRA adapters not applied properly.\")\n",
    "        \n",
    "        # Move model to device AFTER parameter counting\n",
    "        model.to(DEVICE)\n",
    "        model.print_trainable_parameters()\n",
    "        \n",
    "        # Print detailed parameter info\n",
    "        print(\"\\nDetailed parameter breakdown:\")\n",
    "        lora_params = 0\n",
    "        classifier_params = 0\n",
    "        other_params = 0\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'lora' in name.lower():\n",
    "                    lora_params += param.numel()\n",
    "                    print(f\"  üîµ LoRA: {name} - {param.numel():,} params\")\n",
    "                elif 'classifier' in name.lower() or 'head' in name.lower():\n",
    "                    classifier_params += param.numel()\n",
    "                    print(f\"  üü¢ Classifier: {name} - {param.numel():,} params\")\n",
    "                else:\n",
    "                    other_params += param.numel()\n",
    "                    print(f\"  ‚ö´ Other: {name} - {param.numel():,} params\")\n",
    "        \n",
    "        print(f\"\\nParameter summary:\")\n",
    "        print(f\"  LoRA parameters: {lora_params:,}\")\n",
    "        print(f\"  Classifier parameters: {classifier_params:,}\")\n",
    "        print(f\"  Other trainable parameters: {other_params:,}\")\n",
    "        print(f\"  Total trainable: {trainable_params:,}\")\n",
    "        \n",
    "        # Initialize optimizer (only trainable parameters)\n",
    "        optimizer_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        print(f\"DEBUG: Optimizer will optimize {len(optimizer_params)} parameter groups\")\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            optimizer_params,\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n‚úÖ LoRA Model initialized successfully\")\n",
    "        print(f\"   Total parameters: {total_params:,}\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:,} ({(trainable_params/total_params)*100:.2f}%)\")\n",
    "        print(f\"   Device: {DEVICE}\")\n",
    "        \n",
    "        # Calculate gradient accumulation steps based on batch size\n",
    "        # Lower batch sizes might benefit from gradient accumulation\n",
    "        gradient_accumulation_steps = 1\n",
    "        if config['batch_size'] >= 64:  # Large batch size, reduce gradient accumulation\n",
    "            gradient_accumulation_steps = 1\n",
    "        elif config['batch_size'] >= 32:\n",
    "            gradient_accumulation_steps = 2\n",
    "        else:\n",
    "            gradient_accumulation_steps = 4\n",
    "        \n",
    "        print(f\"   Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "        \n",
    "        # Clear memory before training\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Train model\n",
    "        model_exp, history_exp, run_folder_exp = train_lora_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train,\n",
    "            val_dataset=valid,\n",
    "            num_epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            device=DEVICE,\n",
    "            model_name=config['name'],\n",
    "            use_wandb=True,\n",
    "            wandb_config={\n",
    "                \"learning_rate\": config['learning_rate'],\n",
    "                \"batch_size\": config['batch_size'],\n",
    "                \"epochs\": config['epochs'],\n",
    "                \"weight_decay\": config['weight_decay'],\n",
    "                \"model_name\": \"vit_base_patch16_224\",\n",
    "                \"architecture\": \"ViT + LoRA\",\n",
    "                \"dataset\": \"FER2013\",\n",
    "                \"transform_set\": config['transform_key'],\n",
    "                \"experiment_name\": config['name'],\n",
    "                \"lora_r\": config['lora_r'],\n",
    "                \"lora_alpha\": config['lora_alpha'],\n",
    "                \"lora_dropout\": config['lora_dropout'],\n",
    "                \"lora_target_modules\": config['target_modules'],\n",
    "                \"trainable_params\": trainable_params,\n",
    "                \"trainable_percentage\": (trainable_params/total_params)*100,\n",
    "                \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "            },\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_lora_results[config['name']] = {\n",
    "            'model': model_exp,\n",
    "            'history': history_exp,\n",
    "            'run_folder': run_folder_exp,\n",
    "            'config': config,\n",
    "            'best_val_accuracy': max(history_exp['val_acc']),\n",
    "            'best_val_loss': min(history_exp['val_loss']),\n",
    "            'final_train_accuracy': history_exp['train_acc'][-1],\n",
    "            'final_train_loss': history_exp['train_loss'][-1],\n",
    "            'lora_adapter_path': run_folder_exp / \"lora_adapter\",\n",
    "            'trainable_params': trainable_params,\n",
    "            'total_params': total_params,\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ COMPLETED: {config['name']}\")\n",
    "        print(f\"   Best Val Accuracy: {all_lora_results[config['name']]['best_val_accuracy']:.4f}\")\n",
    "        print(f\"   Best Val Loss: {all_lora_results[config['name']]['best_val_loss']:.4f}\")\n",
    "        print(f\"   Run folder: {run_folder_exp}\")\n",
    "        \n",
    "        # Save summary to file\n",
    "        summary_path = run_folder_exp / \"training_summary.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(f\"LoRA Experiment: {config['name']}\\n\")\n",
    "            f.write(f\"Transform: {config['transform_key']}\\n\")\n",
    "            f.write(f\"Learning Rate: {config['learning_rate']}\\n\")\n",
    "            f.write(f\"Epochs: {config['epochs']}\\n\")\n",
    "            f.write(f\"Batch Size: {config['batch_size']}\\n\")\n",
    "            f.write(f\"LoRA Rank: {config['lora_r']}\\n\")\n",
    "            f.write(f\"LoRA Alpha: {config['lora_alpha']}\\n\")\n",
    "            f.write(f\"LoRA Dropout: {config['lora_dropout']}\\n\")\n",
    "            f.write(f\"Target Modules: {config['target_modules']}\\n\")\n",
    "            f.write(f\"Best Val Accuracy: {all_lora_results[config['name']]['best_val_accuracy']:.4f}\\n\")\n",
    "            f.write(f\"Best Val Loss: {all_lora_results[config['name']]['best_val_loss']:.4f}\\n\")\n",
    "            f.write(f\"Final Train Accuracy: {history_exp['train_acc'][-1]:.4f}\\n\")\n",
    "            f.write(f\"Final Train Loss: {history_exp['train_loss'][-1]:.4f}\\n\")\n",
    "            f.write(f\"Trainable Parameters: {trainable_params:,}\\n\")\n",
    "            f.write(f\"Total Parameters: {total_params:,}\\n\")\n",
    "            f.write(f\"Trainable Percentage: {(trainable_params/total_params)*100:.2f}%\\n\")\n",
    "        \n",
    "        print(f\"   Summary saved to: {summary_path}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n‚ö†Ô∏è Training interrupted by user at experiment: {config['name']}\")\n",
    "        cleanup_wandb_run()\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå ERROR in experiment {config['name']}: {str(e)}\")\n",
    "        print(f\"Exception type: {type(e).__name__}\")\n",
    "        \n",
    "        # Store failed experiment info\n",
    "        failed_lora_experiments.append({\n",
    "            'name': config['name'],\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__,\n",
    "            'traceback': traceback.format_exc() if 'traceback' in locals() else \"N/A\"\n",
    "        })\n",
    "        \n",
    "        # Clean up WandB\n",
    "        cleanup_wandb_run()\n",
    "        \n",
    "        print(f\"Continuing to next experiment...\")\n",
    "        \n",
    "    finally:\n",
    "        # Comprehensive memory cleanup\n",
    "        print(\"\\nüßπ Performing comprehensive memory cleanup...\")\n",
    "        \n",
    "        # Delete all local variables\n",
    "        local_vars = list(locals().keys())\n",
    "        for var_name in ['model', 'base_model', 'model_exp', 'optimizer', 'train', 'valid', 'lora_config']:\n",
    "            if var_name in locals():\n",
    "                del locals()[var_name]\n",
    "                print(f\"   Deleted: {var_name}\")\n",
    "        \n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        \n",
    "        # Clear CUDA cache if available\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            # Print memory stats\n",
    "            allocated = torch.cuda.memory_allocated() / 1e9\n",
    "            reserved = torch.cuda.memory_reserved() / 1e9\n",
    "            print(f\"   GPU Memory after cleanup: {allocated:.2f} GB allocated, {reserved:.2f} GB reserved\")\n",
    "        \n",
    "        print(\"‚úÖ Memory cleaned up\")\n",
    "        \n",
    "        # Add a small delay between experiments\n",
    "        import time\n",
    "        time.sleep(2)  # 2 second delay to ensure proper cleanup\n",
    "\n",
    "# Final cleanup\n",
    "cleanup_wandb_run()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LORA TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Successful experiments: {len(all_lora_results)}/{len(lora_experiment_configs)}\")\n",
    "print(f\"Failed experiments: {len(failed_lora_experiments)}/{len(lora_experiment_configs)}\")\n",
    "\n",
    "if all_lora_results:\n",
    "    print(\"\\nüìä LoRA Results (sorted by Best Val Accuracy):\")\n",
    "    \n",
    "    # Sort results by best validation accuracy\n",
    "    sorted_results = sorted(\n",
    "        all_lora_results.items(),\n",
    "        key=lambda x: x[1]['best_val_accuracy'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for name, result in sorted_results:\n",
    "        config = result['config']\n",
    "        print(f\"\\n   {name}:\")\n",
    "        print(f\"      Val Acc: {result['best_val_accuracy']:.4f}\")\n",
    "        print(f\"      Val Loss: {result['best_val_loss']:.4f}\")\n",
    "        print(f\"      Train Acc: {result['final_train_accuracy']:.4f}\")\n",
    "        print(f\"      LR: {config['learning_rate']}\")\n",
    "        print(f\"      LoRA Rank: {config['lora_r']}\")\n",
    "        print(f\"      Target Modules: {config['target_modules']}\")\n",
    "        print(f\"      Trainable Params: {result['trainable_params']:,}\")\n",
    "        print(f\"      Trainable %: {(result['trainable_params']/result['total_params'])*100:.2f}%\")\n",
    "        print(f\"      Run Folder: {result['run_folder'].name}\")\n",
    "\n",
    "# Print failed experiments\n",
    "if failed_lora_experiments:\n",
    "    print(\"\\n‚ùå Failed Experiments:\")\n",
    "    for failed in failed_lora_experiments:\n",
    "        print(f\"\\n   {failed['name']}:\")\n",
    "        print(f\"      Error Type: {failed['error_type']}\")\n",
    "        print(f\"      Error: {failed['error']}\")\n",
    "\n",
    "# Save overall results to file\n",
    "if all_lora_results:\n",
    "    import json\n",
    "    from datetime import datetime\n",
    "    \n",
    "    results_summary = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'total_experiments': len(lora_experiment_configs),\n",
    "        'successful': len(all_lora_results),\n",
    "        'failed': len(failed_lora_experiments),\n",
    "        'results': {},\n",
    "        'failed_experiments': failed_lora_experiments,\n",
    "    }\n",
    "    \n",
    "    for name, result in all_lora_results.items():\n",
    "        results_summary['results'][name] = {\n",
    "            'best_val_accuracy': float(result['best_val_accuracy']),\n",
    "            'best_val_loss': float(result['best_val_loss']),\n",
    "            'final_train_accuracy': float(result['final_train_accuracy']),\n",
    "            'final_train_loss': float(result['final_train_loss']),\n",
    "            'trainable_params': result['trainable_params'],\n",
    "            'total_params': result['total_params'],\n",
    "            'run_folder': str(result['run_folder']),\n",
    "            'config': result['config'],\n",
    "        }\n",
    "    \n",
    "    # Save to JSON\n",
    "    import os\n",
    "    results_dir = Path(\"checkpoints\") / \"lora_experiments_summary\"\n",
    "    results_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_file = results_dir / f\"lora_experiments_summary_{timestamp}.json\"\n",
    "    \n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüìÅ Full results summary saved to: {results_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7db16",
   "metadata": {},
   "source": [
    "### Inference\n",
    "#### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc73fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Find best LoRA model\n",
    "if all_lora_results:\n",
    "    best_lora_name = max(all_lora_results, key=lambda x: all_lora_results[x]['best_val_accuracy'])\n",
    "    best_lora_result = all_lora_results[best_lora_name]\n",
    "    \n",
    "    print(f\"Loading best LoRA model: {best_lora_name}\")\n",
    "    print(f\"Best validation accuracy: {best_lora_result['best_val_accuracy']:.4f}\")\n",
    "    print(f\"LoRA adapter path: {best_lora_result['lora_adapter_path']}\")\n",
    "    \n",
    "    # Load base model\n",
    "    base_model_for_inference = ViTForImageClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapters\n",
    "    lora_model_for_inference = load_lora_model(\n",
    "        base_model_for_inference,\n",
    "        best_lora_result['lora_adapter_path'],\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ LoRA model ready for inference!\")\n",
    "else:\n",
    "    print(\"‚ùå No LoRA models trained yet. Run the training cell first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d841652c",
   "metadata": {},
   "source": [
    "#### Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce180e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metadata import find_latest_run_for_experiment\n",
    "from transformers import ViTImageProcessor\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "from src.evaluate import evaluate_model\n",
    "from src.lora import load_lora_model\n",
    "from src.config import DEVICE, EMOTION_LABELS, NUM_LABELS\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# Initialize test dataset\n",
    "test_ds = FER2013Dataset(split=\"test\", transform=base_transform())\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Define your checkpoints directory\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "# Convert EMOTION_LABELS list to dict\n",
    "EMOTION_DICT = {i: emotion for i, emotion in enumerate(EMOTION_LABELS)}\n",
    "\n",
    "print(\"üîç LoRA Model Evaluation - UPDATED VERSION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "print(f\"Emotion labels: {EMOTION_LABELS}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def load_lora_model_correctly_for_evaluation(model_folder):\n",
    "    \"\"\"\n",
    "    CORRECT way to load a LoRA model for evaluation.\n",
    "    \n",
    "    Must load BOTH:\n",
    "    1. LoRA adapters (from lora_adapter/)\n",
    "    2. Classifier weights (from .pth checkpoint)\n",
    "    \"\"\"\n",
    "    # Find the latest run for this experiment\n",
    "    run_folder = find_latest_run_for_experiment(model_folder, CHECKPOINTS_DIR)\n",
    "    if not run_folder:\n",
    "        print(f\"‚ùå No run folder found for {model_folder}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìÅ Loading from: {model_folder}\")\n",
    "    print(f\"   Run folder: {run_folder.name}\")\n",
    "    \n",
    "    # Step 1: Find the .pth checkpoint (has classifier weights)\n",
    "    pth_files = list(run_folder.glob(\"*best*.pth\"))\n",
    "    if not pth_files:\n",
    "        print(f\"   ‚ùå No .pth checkpoint found in {run_folder}\")\n",
    "        return None\n",
    "    \n",
    "    pth_path = pth_files[0]\n",
    "    print(f\"   ‚úì Found checkpoint: {pth_path.name}\")\n",
    "    \n",
    "    # Step 2: Check if lora_adapter folder exists\n",
    "    lora_adapter_path = run_folder / \"lora_adapter\"\n",
    "    if not lora_adapter_path.exists():\n",
    "        print(f\"   ‚ùå lora_adapter folder not found at: {lora_adapter_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   ‚úì Found lora_adapter folder\")\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Load base model\n",
    "        print(f\"   Loading base ViT model...\")\n",
    "        base_model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: Load LoRA adapters using your existing function\n",
    "        print(f\"   Loading LoRA adapters...\")\n",
    "        model = load_lora_model(\n",
    "            base_model=base_model,\n",
    "            lora_adapter_path=str(lora_adapter_path),\n",
    "            device='cpu'  # Load to CPU first\n",
    "        )\n",
    "        \n",
    "        # Step 5: Merge LoRA weights into base model\n",
    "        print(f\"   Merging LoRA weights...\")\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # Step 6: Load classifier weights from .pth checkpoint\n",
    "        print(f\"   Loading classifier weights from checkpoint...\")\n",
    "        checkpoint = torch.load(pth_path, map_location='cpu', weights_only=False)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Extract classifier weights - handle both naming formats\n",
    "        classifier_weight = None\n",
    "        classifier_bias = None\n",
    "        \n",
    "        # Try different naming patterns\n",
    "        possible_weight_keys = [\n",
    "            'base_model.model.classifier.weight',\n",
    "            'classifier.weight',\n",
    "            'model.classifier.weight'\n",
    "        ]\n",
    "        \n",
    "        possible_bias_keys = [\n",
    "            'base_model.model.classifier.bias',\n",
    "            'classifier.bias',\n",
    "            'model.classifier.bias'\n",
    "        ]\n",
    "        \n",
    "        for key in possible_weight_keys:\n",
    "            if key in state_dict:\n",
    "                classifier_weight = state_dict[key]\n",
    "                print(f\"   Found classifier weight: {key}\")\n",
    "                break\n",
    "        \n",
    "        for key in possible_bias_keys:\n",
    "            if key in state_dict:\n",
    "                classifier_bias = state_dict[key]\n",
    "                print(f\"   Found classifier bias: {key}\")\n",
    "                break\n",
    "        \n",
    "        if classifier_weight is not None and classifier_bias is not None:\n",
    "            # Load classifier weights into the merged model\n",
    "            model.classifier.weight.data = classifier_weight\n",
    "            model.classifier.bias.data = classifier_bias\n",
    "            print(f\"   ‚úÖ Classifier weights loaded!\")\n",
    "            print(f\"      Weight shape: {classifier_weight.shape}\")\n",
    "            print(f\"      Bias shape: {classifier_bias.shape}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: Classifier weights not found in checkpoint!\")\n",
    "            print(f\"      Looking for keys containing 'classifier':\")\n",
    "            classifier_keys = [k for k in state_dict.keys() if 'classifier' in k.lower()]\n",
    "            for key in classifier_keys:\n",
    "                print(f\"      - {key}\")\n",
    "        \n",
    "        # Step 7: Move to device and set to eval mode\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "        \n",
    "        # Verify model structure\n",
    "        print(f\"   ‚úÖ Model loaded successfully!\")\n",
    "        print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        # Quick check of classifier weights\n",
    "        if hasattr(model, 'classifier'):\n",
    "            weight_mean = model.classifier.weight.data.mean().item()\n",
    "            weight_std = model.classifier.weight.data.std().item()\n",
    "            print(f\"   Classifier weight stats: mean={weight_mean:.6f}, std={weight_std:.6f}\")\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading LoRA model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Configuration\n",
    "LORA_MODELS_TO_TEST = [\n",
    "    \"lora_r4_light1\",\n",
    "    \"lora_r8_medium\",\n",
    "    \"lora_r8_qkv_heavy\",\n",
    "    \"lora_r16_medium\",\n",
    "]\n",
    "\n",
    "NUM_SAMPLES = 3  # Number of random test samples per model\n",
    "sample_indices = random.sample(range(len(test_ds)), NUM_SAMPLES)\n",
    "print(f\"\\nRandom sample indices for visual inspection: {sample_indices}\")\n",
    "\n",
    "# Load processor for image preprocessing\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def display_model_predictions(model_name, results, sample_indices):\n",
    "    \"\"\"Display predictions for a single model.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    correct_count = sum(1 for r in results if r['correct'])\n",
    "    accuracy = correct_count / len(results)\n",
    "    \n",
    "    print(f\"Batch Accuracy: {correct_count}/{len(results)} ({accuracy:.1%})\")\n",
    "    print(f\"Samples tested: {sample_indices}\")\n",
    "    print()\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Sample {i+1} (Index {result['sample_index']}):\")\n",
    "        print(f\"  True: {EMOTION_DICT[result['true_label']]:<12}\", end=\"\")\n",
    "        print(f\"  Predicted: {EMOTION_DICT[result['pred_label']]:<12}\", end=\"\")\n",
    "        print(f\"  Confidence: {result['confidence']:.1%}\", end=\"\")\n",
    "        print(f\"  {'‚úì' if result['correct'] else '‚úó'}\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        print(f\"  Top 3: \", end=\"\")\n",
    "        for j, (emotion_idx, prob) in enumerate(result['top3_predictions']):\n",
    "            emotion_name = EMOTION_DICT[emotion_idx]\n",
    "            print(f\"{emotion_name}: {prob:.1%}\", end=\"\")\n",
    "            if j < 2:\n",
    "                print(\", \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize all samples in a grid\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(4*len(results), 4))\n",
    "    if len(results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (result, ax) in enumerate(zip(results, axes)):\n",
    "        ax.imshow(result['image'], cmap='gray')\n",
    "        title = f\"Sample {i+1}\"\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Model: {model_name}\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def predict_lora_batch(dataset, indices, model, model_name):\n",
    "    \"\"\"Run predictions on multiple samples for a LoRA model.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        img, true_label = dataset[idx]\n",
    "        \n",
    "        # Convert tensor to numpy for display\n",
    "        img_display = img.cpu().numpy()\n",
    "        if len(img_display.shape) == 3:\n",
    "            img_display = img_display.transpose(1, 2, 0)\n",
    "            # If RGB, convert to grayscale for display\n",
    "            if img_display.shape[2] == 3:\n",
    "                img_display = img_display.mean(axis=2)\n",
    "        \n",
    "        # Run model\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        img_batch = img.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=img_batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "            pred_label = torch.argmax(probs).item()\n",
    "            confidence = probs[pred_label].item()\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top3_probs, top3_idx = torch.topk(probs, 3)\n",
    "        top3_predictions = [\n",
    "            (idx.item(), prob.item()) \n",
    "            for prob, idx in zip(top3_probs, top3_idx)\n",
    "        ]\n",
    "        \n",
    "        results.append({\n",
    "            'sample_index': idx,\n",
    "            'true_label': true_label,\n",
    "            'pred_label': pred_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == pred_label,\n",
    "            'top3_predictions': top3_predictions,\n",
    "            'image': img_display\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(f\"\\nTesting {len(LORA_MODELS_TO_TEST)} LoRA models\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "lora_model_results = {}\n",
    "full_evaluation_results = {}\n",
    "\n",
    "for lora_name in LORA_MODELS_TO_TEST:\n",
    "    try:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing: {lora_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Load the model using CORRECT method\n",
    "        model = load_lora_model_correctly_for_evaluation(lora_name)\n",
    "        \n",
    "        if model is None:\n",
    "            print(f\"‚ùå Failed to load model\")\n",
    "            continue\n",
    "        \n",
    "        # 1. Quick sanity check on random samples\n",
    "        print(f\"\\nüß™ Running quick visual inspection on {NUM_SAMPLES} samples...\")\n",
    "        visual_results = predict_lora_batch(\n",
    "            dataset=test_ds,\n",
    "            indices=sample_indices,\n",
    "            model=model,\n",
    "            model_name=lora_name\n",
    "        )\n",
    "        \n",
    "        visual_accuracy = display_model_predictions(lora_name, visual_results, sample_indices)\n",
    "        \n",
    "        # 2. Run full evaluation\n",
    "        print(f\"\\nüìä Running full evaluation on entire test set...\")\n",
    "        metrics = evaluate_model(\n",
    "            model=model,\n",
    "            test_dataset=test_ds,\n",
    "            log_to_wandb=False,\n",
    "            run_name=lora_name.replace(\"_\", \" \").title()\n",
    "        )\n",
    "        \n",
    "        # Store both results\n",
    "        lora_model_results[lora_name] = {\n",
    "            'visual_accuracy': visual_accuracy,\n",
    "            'correct': sum(1 for r in visual_results if r['correct']),\n",
    "            'total': len(visual_results),\n",
    "            'full_accuracy': metrics.get('accuracy', 0)\n",
    "        }\n",
    "        \n",
    "        full_evaluation_results[lora_name] = {\n",
    "            'metrics': metrics,\n",
    "            'visual_accuracy': visual_accuracy\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        display_name = lora_name.replace(\"_\", \" \").title()\n",
    "        print(f\"\\n‚úÖ Results for {display_name}:\")\n",
    "        print(f\"   ‚Ä¢ Visual check ({NUM_SAMPLES} samples): {visual_accuracy:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Full test accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Precision: {metrics.get('precision', 0):.4f}\")\n",
    "        print(f\"   ‚Ä¢ Recall: {metrics.get('recall', 0):.4f}\")\n",
    "        print(f\"   ‚Ä¢ F1 Score: {metrics.get('f1', 0):.4f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error evaluating {lora_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# Print summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: LoRA Model Comparison\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if lora_model_results:\n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for model_name, results in lora_model_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name.replace(\"_\", \" \").title(),\n",
    "            'Visual Accuracy': results['visual_accuracy'],\n",
    "            'Full Test Accuracy': results['full_accuracy'],\n",
    "            'Visual Correct': f\"{results['correct']}/{results['total']}\"\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by full test accuracy\n",
    "    df_comparison = df_comparison.sort_values('Full Test Accuracy', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    df_display = df_comparison.copy()\n",
    "    for col in ['Visual Accuracy', 'Full Test Accuracy']:\n",
    "        df_display[col] = df_display[col].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + df_display.to_string(index=False))\n",
    "    \n",
    "    # Print ranking\n",
    "    print(f\"\\nüèÜ Performance Ranking:\")\n",
    "    for i, (_, row) in enumerate(df_comparison.iterrows()):\n",
    "        print(f\"{i+1}. {row['Model']:<25} \"\n",
    "              f\"Full Acc: {row['Full Test Accuracy']:.4f} | \"\n",
    "              f\"Visual: {row['Visual Correct']} ({row['Visual Accuracy']:.1%})\")\n",
    "    \n",
    "    best_model = df_comparison.iloc[0]\n",
    "    print(f\"\\nüèÜ Best LoRA Model: {best_model['Model']} \"\n",
    "          f\"(Full Accuracy: {best_model['Full Test Accuracy']:.4f})\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    csv_path = CHECKPOINTS_DIR / \"lora_comparison_results.csv\"\n",
    "    df_comparison.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nüíæ Results saved to: {csv_path}\")\n",
    "    \n",
    "    # Check if any models have suspiciously low performance\n",
    "    low_perf_models = df_comparison[df_comparison['Full Test Accuracy'] < 0.3]\n",
    "    if not low_perf_models.empty:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Models with low performance (< 30%):\")\n",
    "        for _, row in low_perf_models.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['Model']}: {row['Full Test Accuracy']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No models were successfully evaluated.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"Models attempted: {len(LORA_MODELS_TO_TEST)}\")\n",
    "print(f\"Models successfully evaluated: {len(lora_model_results)}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0627066",
   "metadata": {},
   "source": [
    "#### Full Test Set Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc66c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.evaluate import evaluate_model\n",
    "from src.lora import load_lora_model_for_inference\n",
    "from src.config import DEVICE, EMOTION_LABELS, NUM_LABELS\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "from src.metadata import find_latest_run_for_experiment\n",
    "\n",
    "# Initialize test dataset\n",
    "test_ds = FER2013Dataset(split=\"test\", transform=base_transform())\n",
    "\n",
    "# Define your checkpoints directory\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "print(\"FULL TEST SET EVALUATION - LoRA Models\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "LORA_MODELS_TO_EVALUATE = [\n",
    "    # \"lora_r4_light_baseline_long\",\n",
    "    # \"lora_r6_light_long\"\n",
    "\n",
    "    \"lora_r4_light_lr\",\n",
    "    \"lora_r32_light_short\",\n",
    "    \"lora_r48_light_focused\"\n",
    "    # \"lora_r4_light_low_dropout\",\n",
    "    # \"lora_r6_light\"\n",
    "]\n",
    "# =========================\n",
    "\n",
    "print(f\"\\nEvaluating {len(LORA_MODELS_TO_EVALUATE)} LoRA models on full test set\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "all_results = {}\n",
    "failed_models = []\n",
    "\n",
    "for model_name in LORA_MODELS_TO_EVALUATE:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Find the latest run\n",
    "        run_folder = find_latest_run_for_experiment(model_name, CHECKPOINTS_DIR)\n",
    "        \n",
    "        # Load model using the new function\n",
    "        print(\"Loading model...\")\n",
    "        model = load_lora_model_for_inference(run_folder, device=DEVICE)\n",
    "        \n",
    "        # Run full evaluation\n",
    "        print(\"Running evaluation on full test set...\")\n",
    "        metrics = evaluate_model(\n",
    "            model=model,\n",
    "            test_dataset=test_ds,\n",
    "            log_to_wandb=False,\n",
    "            run_name=model_name.replace(\"_\", \" \").title()\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[model_name] = {\n",
    "            'accuracy': metrics.get('accuracy', 0),\n",
    "            'precision': metrics.get('precision', 0),\n",
    "            'recall': metrics.get('recall', 0),\n",
    "            'f1': metrics.get('f1', 0),\n",
    "            'run_folder': str(run_folder),\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        display_name = model_name.replace(\"_\", \" \").title()\n",
    "        print(f\"\\nResults for {display_name}:\")\n",
    "        print(f\"  Accuracy:  {metrics.get('accuracy', 0):.4f}\")\n",
    "        print(f\"  Precision: {metrics.get('precision', 0):.4f}\")\n",
    "        print(f\"  Recall:    {metrics.get('recall', 0):.4f}\")\n",
    "        print(f\"  F1 Score:  {metrics.get('f1', 0):.4f}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name}: {e}\")\n",
    "        failed_models.append(model_name)\n",
    "        continue\n",
    "\n",
    "# Generate final report\n",
    "if all_results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"FINAL EVALUATION REPORT - FULL TEST SET\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_data = []\n",
    "    for model_name, results in all_results.items():\n",
    "        comparison_data.append({\n",
    "            'Model': model_name.replace(\"_\", \" \").title(),\n",
    "            'Accuracy': results['accuracy'],\n",
    "            'Precision': results['precision'],\n",
    "            'Recall': results['recall'],\n",
    "            'F1 Score': results['f1'],\n",
    "            'Run Folder': Path(results['run_folder']).name\n",
    "        })\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    df_comparison = df_comparison.sort_values('Accuracy', ascending=False)\n",
    "    \n",
    "    # Format for display\n",
    "    df_display = df_comparison.copy()\n",
    "    for col in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:\n",
    "        df_display[col] = df_display[col].apply(lambda x: f\"{x:.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + df_display.to_string(index=False))\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = df_comparison.iloc[0]\n",
    "    print(f\"\\nBEST PERFORMING MODEL:\")\n",
    "    print(f\"  Name: {best_model['Model']}\")\n",
    "    print(f\"  Accuracy: {best_model['Accuracy']:.4f}\")\n",
    "    print(f\"  F1 Score: {best_model['F1 Score']:.4f}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    accuracies = df_comparison['Accuracy'].tolist()\n",
    "    print(f\"\\nSTATISTICS:\")\n",
    "    print(f\"  Average Accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"  Best Accuracy:    {np.max(accuracies):.4f}\")\n",
    "    print(f\"  Worst Accuracy:   {np.min(accuracies):.4f}\")\n",
    "    print(f\"  Std Deviation:    {np.std(accuracies):.4f}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    csv_path = CHECKPOINTS_DIR / \"lora_full_evaluation_results.csv\"\n",
    "    df_comparison.to_csv(csv_path, index=False)\n",
    "    print(f\"\\nFull results saved to: {csv_path}\")\n",
    "    \n",
    "    # Identify underperforming models\n",
    "    low_perf_threshold = 0.3\n",
    "    low_perf_models = df_comparison[df_comparison['Accuracy'] < low_perf_threshold]\n",
    "    \n",
    "    if not low_perf_models.empty:\n",
    "        print(f\"\\nMODELS WITH LOW PERFORMANCE (<{low_perf_threshold:.0%}):\")\n",
    "        for _, row in low_perf_models.iterrows():\n",
    "            print(f\"  {row['Model']}: {row['Accuracy']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No models were successfully evaluated.\")\n",
    "\n",
    "# Report failed models\n",
    "if failed_models:\n",
    "    print(f\"\\nFAILED TO EVALUATE ({len(failed_models)}):\")\n",
    "    for model in failed_models:\n",
    "        print(f\"  {model}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"  Total models: {len(LORA_MODELS_TO_EVALUATE)}\")\n",
    "print(f\"  Successfully evaluated: {len(all_results)}\")\n",
    "print(f\"  Failed: {len(failed_models)}\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a2d5e",
   "metadata": {},
   "source": [
    "#### Visual Insepction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from src.lora import load_lora_model_for_inference\n",
    "from src.config import DEVICE, EMOTION_LABELS, NUM_LABELS\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "from src.metadata import find_latest_run_for_experiment\n",
    "\n",
    "# Initialize test dataset\n",
    "test_ds = FER2013Dataset(split=\"test\", transform=base_transform())\n",
    "\n",
    "# Define your checkpoints directory\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "# Convert EMOTION_LABELS list to dict\n",
    "EMOTION_DICT = {i: emotion for i, emotion in enumerate(EMOTION_LABELS)}\n",
    "\n",
    "print(\"VISUAL INSPECTION - LoRA Models on N Samples\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def display_predictions(model_name, results, sample_indices):\n",
    "    \"\"\"Display predictions in a clean format.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    correct_count = sum(1 for r in results if r['correct'])\n",
    "    accuracy = correct_count / len(results)\n",
    "    \n",
    "    print(f\"Accuracy on {len(results)} samples: {correct_count}/{len(results)} ({accuracy:.1%})\")\n",
    "    print(f\"Sample indices: {sample_indices}\")\n",
    "    print()\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        status = \"‚úì\" if result['correct'] else \"‚úó\"\n",
    "        print(f\"Sample {i+1} (Index {result['sample_index']}): {status}\")\n",
    "        print(f\"  True:      {EMOTION_DICT[result['true_label']]:<12}\")\n",
    "        print(f\"  Predicted: {EMOTION_DICT[result['pred_label']]:<12} ({result['confidence']:.1%})\")\n",
    "        \n",
    "        print(f\"  Top predictions:\", end=\"\")\n",
    "        for j, (emotion_idx, prob) in enumerate(result['top3_predictions'][:3]):\n",
    "            emotion_name = EMOTION_DICT[emotion_idx]\n",
    "            print(f\" {emotion_name}: {prob:.1%}\", end=\"\")\n",
    "            if j < 2:\n",
    "                print(\",\", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def visualize_samples(results, model_name):\n",
    "    \"\"\"Create visualization of predictions.\"\"\"\n",
    "    n_samples = len(results)\n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(4*n_samples, 4))\n",
    "    \n",
    "    if n_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (result, ax) in enumerate(zip(results, axes)):\n",
    "        ax.imshow(result['image'], cmap='gray')\n",
    "        \n",
    "        border_color = 'green' if result['correct'] else 'red'\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_color(border_color)\n",
    "            spine.set_linewidth(3)\n",
    "        \n",
    "        true_label = EMOTION_DICT[result['true_label']]\n",
    "        pred_label = EMOTION_DICT[result['pred_label']]\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        title = f\"Sample {i+1}\\nTrue: {true_label}\\nPred: {pred_label} ({confidence:.0%})\"\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Model: {model_name}\", fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def predict_samples(model, dataset, indices):\n",
    "    \"\"\"Run predictions on specific samples.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        img, true_label = dataset[idx]\n",
    "        \n",
    "        # Prepare image for display\n",
    "        img_display = img.cpu().numpy()\n",
    "        if len(img_display.shape) == 3:\n",
    "            img_display = img_display.transpose(1, 2, 0)\n",
    "            if img_display.shape[2] == 3:\n",
    "                img_display = img_display.mean(axis=2)\n",
    "        \n",
    "        # Run inference\n",
    "        model.eval()\n",
    "        img_batch = img.unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(pixel_values=img_batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "            pred_label = torch.argmax(probs).item()\n",
    "            confidence = probs[pred_label].item()\n",
    "        \n",
    "        # Get top predictions\n",
    "        top3_probs, top3_idx = torch.topk(probs, 3)\n",
    "        top3_predictions = [(idx.item(), prob.item()) for prob, idx in zip(top3_probs, top3_idx)]\n",
    "        \n",
    "        results.append({\n",
    "            'sample_index': idx,\n",
    "            'true_label': true_label,\n",
    "            'pred_label': pred_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == pred_label,\n",
    "            'top3_predictions': top3_predictions,\n",
    "            'image': img_display\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "LORA_MODELS_TO_INSPECT = [\n",
    "    \"lora_r4_light1\",\n",
    "    \"lora_r8_medium\",\n",
    "    \"lora_r8_qkv_heavy\",\n",
    "    \"lora_r16_medium\",\n",
    "]\n",
    "\n",
    "NUM_SAMPLES = 3  # Number of random samples to inspect\n",
    "# =========================\n",
    "\n",
    "print(f\"\\nVisual inspection of {len(LORA_MODELS_TO_INSPECT)} LoRA models\")\n",
    "print(f\"Testing {NUM_SAMPLES} random samples per model\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get random samples (same for all models for fair comparison)\n",
    "sample_indices = random.sample(range(len(test_ds)), NUM_SAMPLES)\n",
    "print(f\"Random sample indices: {sample_indices}\")\n",
    "\n",
    "inspection_results = {}\n",
    "\n",
    "for model_name in LORA_MODELS_TO_INSPECT:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Find the latest run\n",
    "        run_folder = find_latest_run_for_experiment(model_name, CHECKPOINTS_DIR)\n",
    "        \n",
    "        # Load model using the new function\n",
    "        model = load_lora_model_for_inference(run_folder, device=DEVICE)\n",
    "        \n",
    "        # Run predictions\n",
    "        print(f\"Running predictions on {NUM_SAMPLES} samples...\")\n",
    "        results = predict_samples(model, test_ds, sample_indices)\n",
    "        \n",
    "        # Display results\n",
    "        accuracy = display_predictions(model_name, results, sample_indices)\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_samples(results, model_name)\n",
    "        \n",
    "        # Store results\n",
    "        inspection_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': sum(1 for r in results if r['correct']),\n",
    "            'total': len(results),\n",
    "            'samples': sample_indices,\n",
    "            'run_folder': str(run_folder)\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during inspection: {e}\")\n",
    "        continue\n",
    "\n",
    "# Print summary\n",
    "if inspection_results:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"VISUAL INSPECTION SUMMARY\")\n",
    "    print('='*80)\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_models = sorted(\n",
    "        inspection_results.items(),\n",
    "        key=lambda x: x[1]['accuracy'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerformance Ranking:\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (model_name, results) in enumerate(sorted_models):\n",
    "        print(f\"{i+1}. {model_name:<25} \"\n",
    "              f\"{results['correct']}/{results['total']} ({results['accuracy']:.1%})\")\n",
    "    \n",
    "    best_model = sorted_models[0]\n",
    "    print(f\"\\nBest in inspection: {best_model[0]} ({best_model[1]['accuracy']:.1%})\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo models were successfully inspected.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"INSPECTION COMPLETE\")\n",
    "print(f\"Models attempted: {len(LORA_MODELS_TO_INSPECT)}\")\n",
    "print(f\"Models inspected: {len(inspection_results)}\")\n",
    "print(f\"Samples per model: {NUM_SAMPLES}\")\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462663a",
   "metadata": {},
   "source": [
    "## Debug \n",
    "### Reload Failed Models from Backup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b370c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct FER2013 dataset loading and display\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset splits\n",
    "print(\"Loading FER2013 dataset directly...\")\n",
    "train_ds = load_dataset(\"AutumnQiu/fer2013\", split=\"train\")\n",
    "val_ds = load_dataset(\"AutumnQiu/fer2013\", split=\"valid\")\n",
    "test_ds = load_dataset(\"AutumnQiu/fer2013\", split=\"test\")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)}\")\n",
    "print(f\"Validation samples: {len(val_ds)}\")\n",
    "print(f\"Test samples: {len(test_ds)}\")\n",
    "\n",
    "# EMOTION_LABELS (in case not imported)\n",
    "EMOTION_LABELS = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "\n",
    "# Display random samples from each split\n",
    "num_samples = 3\n",
    "splits = [(\"Train\", train_ds), (\"Validation\", val_ds), (\"Test\", test_ds)]\n",
    "\n",
    "for split_name, dataset in splits:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{split_name} Split - Random Samples\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get random indices\n",
    "    sample_indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(4*num_samples, 4))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        item = dataset[idx]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        \n",
    "        print(f\"\\nSample {i+1} (Index {idx}):\")\n",
    "        print(f\"  Label: {label} ({EMOTION_LABELS[label]})\")\n",
    "        print(f\"  Image type: {type(img)}\")\n",
    "        \n",
    "        # Convert to PIL if needed\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_pil = img\n",
    "        else:\n",
    "            # Handle numpy array or tensor\n",
    "            img_pil = Image.fromarray(np.array(img))\n",
    "        \n",
    "        print(f\"  PIL mode: {img_pil.mode}\")\n",
    "        print(f\"  PIL size: {img_pil.size}\")\n",
    "        \n",
    "        # Display\n",
    "        axes[i].imshow(img_pil, cmap='gray')\n",
    "        axes[i].set_title(f\"{split_name}\\n{EMOTION_LABELS[label]}\\nMode: {img_pil.mode}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"{split_name} Split - {len(dataset)} samples\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Investigate image properties\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Dataset Properties Investigation\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Check first few samples in test set\n",
    "print(\"\\nFirst 5 test samples:\")\n",
    "for i in range(min(5, len(test_ds))):\n",
    "    item = test_ds[i]\n",
    "    img = item[\"image\"]\n",
    "    label = item[\"label\"]\n",
    "    \n",
    "    if isinstance(img, Image.Image):\n",
    "        mode = img.mode\n",
    "        size = img.size\n",
    "        img_array = np.array(img)\n",
    "    else:\n",
    "        img_array = np.array(img)\n",
    "        mode = f\"Array shape: {img_array.shape}\"\n",
    "        size = f\"Array dtype: {img_array.dtype}\"\n",
    "    \n",
    "    print(f\"  Sample {i}: Label {label} ({EMOTION_LABELS[label]})\")\n",
    "    print(f\"    Image: {mode}, {size}\")\n",
    "    if hasattr(img_array, 'shape'):\n",
    "        print(f\"    Min/Max: {img_array.min()}/{img_array.max()}\")\n",
    "    print()\n",
    "\n",
    "# Show a specific problematic sample if you know the index\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Specific Sample Inspection (if you have problem indices)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# If you had problem indices like [182, 215, 178], check them:\n",
    "problem_indices = [182, 215, 178]\n",
    "print(f\"\\nChecking indices: {problem_indices}\")\n",
    "\n",
    "for idx in problem_indices:\n",
    "    if idx < len(test_ds):\n",
    "        item = test_ds[idx]\n",
    "        img = item[\"image\"]\n",
    "        label = item[\"label\"]\n",
    "        \n",
    "        # Convert to consistent format for display\n",
    "        if isinstance(img, Image.Image):\n",
    "            img_pil = img\n",
    "        else:\n",
    "            img_pil = Image.fromarray(np.array(img))\n",
    "        \n",
    "        # Convert to grayscale for display\n",
    "        if img_pil.mode == 'RGB':\n",
    "            img_display = img_pil.convert('L')\n",
    "            print(f\"Index {idx}: Converted RGB -> Grayscale\")\n",
    "        else:\n",
    "            img_display = img_pil\n",
    "        \n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img_display, cmap='gray')\n",
    "        plt.title(f\"Index {idx}: {EMOTION_LABELS[label]}\\nOriginal: {img_pil.mode}, Display: {img_display.mode}\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Index {idx} out of range (test set size: {len(test_ds)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102718f0",
   "metadata": {},
   "source": [
    "### Dataset Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be0529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick debug: Show de-normalized images\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# ImageNet stats for ViT normalization\n",
    "IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
    "IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(3,1,1)\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    \"\"\"\n",
    "    Undo ImageNet normalization.\n",
    "    Input is a tensor of shape [3, H, W].\n",
    "    Output is a tensor in [0,1] range for visualization.\n",
    "    \"\"\"\n",
    "    img = img_tensor.clone()\n",
    "    img = img * IMAGENET_STD + IMAGENET_MEAN\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"train\",\n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(\"Quick Debug: De-normalized Dataset Images\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "problem_indices = [1253, 417, 1863]  # Your indices\n",
    "\n",
    "fig, axes = plt.subplots(1, len(problem_indices), figsize=(4*len(problem_indices), 4))\n",
    "if len(problem_indices) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, idx in enumerate(problem_indices):\n",
    "    img_tensor, true_label = test_ds[idx]\n",
    "\n",
    "    # De-normalize BEFORE converting to PIL\n",
    "    img_tensor_denorm = denormalize(img_tensor)\n",
    "\n",
    "    img_pil = transforms.ToPILImage()(img_tensor_denorm)\n",
    "\n",
    "    print(f\"Index {idx}:\")\n",
    "    print(f\"  Tensor shape: {img_tensor.shape}\")\n",
    "    print(f\"  PIL mode: {img_pil.mode}\")\n",
    "\n",
    "    axes[i].imshow(img_pil)  # don't force grayscale\n",
    "    axes[i].set_title(f\"Index {idx}\\nLabel: {EMOTION_LABELS[true_label]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"De-Normalized Dataset Images\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644388ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  # <-- ADD THIS\n",
    "\n",
    "# Quick debug without transforms - handle PIL Images\n",
    "test_ds_no_transform = FER2013Dataset(split=\"train\", transform=None)\n",
    "\n",
    "print(\"Quick Debug: Raw Images WITHOUT transforms\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "problem_indices = [1253, 417, 1863]\n",
    "fig, axes = plt.subplots(1, len(problem_indices), figsize=(4*len(problem_indices), 4))\n",
    "if len(problem_indices) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, idx in enumerate(problem_indices):\n",
    "    img, true_label = test_ds_no_transform[idx]\n",
    "    \n",
    "    print(f\"Index {idx}:\")\n",
    "    \n",
    "    # Handle PIL Image or numpy array\n",
    "    if isinstance(img, Image.Image):\n",
    "        print(f\"  PIL Image mode: {img.mode}\")\n",
    "        print(f\"  PIL Image size: {img.size}\")\n",
    "        img_array = np.array(img)\n",
    "    else:\n",
    "        img_array = img\n",
    "        print(f\"  Array shape: {img_array.shape}\")\n",
    "        print(f\"  Array type: {type(img_array)}\")\n",
    "    \n",
    "    # Display\n",
    "    if len(img_array.shape) == 3 and img_array.shape[2] == 3:\n",
    "        # RGB image - take first channel for grayscale display\n",
    "        img_to_show = img_array[:, :, 0]\n",
    "    elif len(img_array.shape) == 2:\n",
    "        # Already grayscale\n",
    "        img_to_show = img_array\n",
    "    else:\n",
    "        # Unknown format, try to display as-is\n",
    "        img_to_show = img_array\n",
    "    \n",
    "    axes[i].imshow(img_to_show, cmap='gray')\n",
    "    axes[i].set_title(f\"Index {idx}\\nLabel: {EMOTION_LABELS[true_label]}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Images WITHOUT transforms\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f0d01",
   "metadata": {},
   "source": [
    "### Lora Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8792d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from src.evaluate import evaluate_model\n",
    "from src.lora import load_lora_model\n",
    "from src.config import DEVICE, EMOTION_LABELS, NUM_LABELS\n",
    "from src.fer2013 import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "from transformers import ViTForImageClassification\n",
    "\n",
    "# Initialize test dataset\n",
    "test_ds = FER2013Dataset(split=\"test\", transform=base_transform())\n",
    "\n",
    "# Define your checkpoints directory\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Convert EMOTION_LABELS list to dict\n",
    "EMOTION_DICT = {i: emotion for i, emotion in enumerate(EMOTION_LABELS)}\n",
    "\n",
    "print(\"üîç LoRA Model Evaluation - FIXED VERSION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Number of labels: {NUM_LABELS}\")\n",
    "print(f\"Emotion labels: {EMOTION_LABELS}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ===== USER: Specify which LoRA folders to evaluate =====\n",
    "LORA_FOLDERS_TO_EVALUATE = [\n",
    "    \"lora_r4_light1\",\n",
    "    # Add more LoRA folder names here\n",
    "]\n",
    "# =========================================================\n",
    "\n",
    "def load_lora_model_correctly(model_folder):\n",
    "    \"\"\"\n",
    "    CORRECT way to load a LoRA model for evaluation.\n",
    "    \n",
    "    Must load BOTH:\n",
    "    1. LoRA adapters (from lora_adapter/)\n",
    "    2. Classifier weights (from .pth checkpoint)\n",
    "    \"\"\"\n",
    "    folder_path = CHECKPOINTS_DIR / model_folder\n",
    "    \n",
    "    if not folder_path.exists():\n",
    "        print(f\"‚ùå Folder not found: {folder_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìÅ Loading from: {model_folder}\")\n",
    "    \n",
    "    # Step 1: Find the .pth checkpoint (has classifier weights)\n",
    "    pth_files = list(folder_path.glob(\"*best*.pth\"))\n",
    "    if not pth_files:\n",
    "        print(f\"   ‚ùå No .pth checkpoint found\")\n",
    "        return None\n",
    "    \n",
    "    pth_path = pth_files[0]\n",
    "    print(f\"   ‚úì Found checkpoint: {pth_path.name}\")\n",
    "    \n",
    "    # Step 2: Check if lora_adapter folder exists\n",
    "    lora_adapter_path = folder_path / \"lora_adapter\"\n",
    "    if not lora_adapter_path.exists():\n",
    "        print(f\"   ‚ùå lora_adapter folder not found at: {lora_adapter_path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   ‚úì Found lora_adapter folder\")\n",
    "    \n",
    "    try:\n",
    "        # Step 3: Load base model\n",
    "        print(f\"   Loading base ViT model...\")\n",
    "        base_model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Step 4: Load LoRA adapters\n",
    "        print(f\"   Loading LoRA adapters...\")\n",
    "        model = load_lora_model(\n",
    "            base_model=base_model,\n",
    "            lora_adapter_path=lora_adapter_path,\n",
    "            device='cpu'  # Load to CPU first\n",
    "        )\n",
    "        \n",
    "        # Step 5: Merge LoRA weights into base model\n",
    "        print(f\"   Merging LoRA weights...\")\n",
    "        model = model.merge_and_unload()\n",
    "        \n",
    "        # Step 6: Load classifier weights from .pth checkpoint\n",
    "        print(f\"   Loading classifier weights from checkpoint...\")\n",
    "        checkpoint = torch.load(pth_path, map_location='cpu', weights_only=False)\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        # Extract classifier weights\n",
    "        classifier_weight = state_dict.get('base_model.model.classifier.weight')\n",
    "        classifier_bias = state_dict.get('base_model.model.classifier.bias')\n",
    "        \n",
    "        if classifier_weight is not None and classifier_bias is not None:\n",
    "            # Load classifier weights into the merged model\n",
    "            model.classifier.weight.data = classifier_weight\n",
    "            model.classifier.bias.data = classifier_bias\n",
    "            print(f\"   ‚úÖ Classifier weights loaded!\")\n",
    "            print(f\"      Weight mean: {classifier_weight.mean().item():.6f}\")\n",
    "            print(f\"      Weight std: {classifier_weight.std().item():.6f}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: Classifier weights not found in checkpoint!\")\n",
    "        \n",
    "        # Step 7: Move to device and set to eval mode\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"   ‚úÖ Model loaded successfully!\")\n",
    "        return model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error loading LoRA model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# Main evaluation loop\n",
    "if LORA_FOLDERS_TO_EVALUATE:\n",
    "    all_results = {}\n",
    "    \n",
    "    for lora_folder in LORA_FOLDERS_TO_EVALUATE:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing: {lora_folder}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the model using CORRECT method\n",
    "            model = load_lora_model_correctly(lora_folder)\n",
    "            \n",
    "            if model is None:\n",
    "                print(f\"‚ùå Failed to load model\")\n",
    "                continue\n",
    "            \n",
    "            # Quick sanity check (32 samples)\n",
    "            print(\"\\nüß™ Running sanity check...\")\n",
    "            test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "            \n",
    "            batch_preds = []\n",
    "            batch_labels = []\n",
    "            batch_confidences = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, labels in test_loader:\n",
    "                    images = images.to(DEVICE)\n",
    "                    outputs = model(pixel_values=images)\n",
    "                    probs = torch.softmax(outputs.logits, dim=-1)\n",
    "                    preds = torch.argmax(outputs.logits, dim=-1)\n",
    "                    \n",
    "                    batch_preds.extend(preds.cpu().numpy())\n",
    "                    batch_labels.extend(labels.numpy())\n",
    "                    batch_confidences.extend(probs.max(dim=-1).values.cpu().numpy())\n",
    "                    \n",
    "                    # Only first batch\n",
    "                    if len(batch_preds) >= 32:\n",
    "                        break\n",
    "            \n",
    "            sanity_acc = accuracy_score(batch_labels, batch_preds)\n",
    "            avg_confidence = np.mean(batch_confidences)\n",
    "            \n",
    "            print(f\"   Sanity check accuracy: {sanity_acc:.4f}\")\n",
    "            print(f\"   Average confidence: {avg_confidence:.4f}\")\n",
    "            \n",
    "            # Check prediction distribution\n",
    "            unique_preds = np.unique(batch_preds)\n",
    "            print(f\"   Predicting {len(unique_preds)} different classes\")\n",
    "            \n",
    "            if len(unique_preds) == 1:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: Model predicts only class {unique_preds[0]} \"\n",
    "                      f\"({EMOTION_DICT.get(unique_preds[0], 'Unknown')})\")\n",
    "            \n",
    "            # If sanity check looks good, proceed with full evaluation\n",
    "            if sanity_acc > 0.3:\n",
    "                print(f\"   ‚úÖ Sanity check passed! Proceeding with full evaluation...\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Sanity check shows low accuracy, but continuing...\")\n",
    "            \n",
    "            # Run full evaluation\n",
    "            print(\"\\nüìä Running full evaluation...\")\n",
    "            metrics = evaluate_model(\n",
    "                model=model,\n",
    "                test_dataset=test_ds,\n",
    "                log_to_wandb=False,\n",
    "                run_name=lora_folder.replace(\"_\", \" \").title()\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            all_results[lora_folder] = {\n",
    "                'metrics': metrics,\n",
    "                'sanity_acc': sanity_acc,\n",
    "                'avg_confidence': avg_confidence\n",
    "            }\n",
    "            \n",
    "            # Print results\n",
    "            display_name = lora_folder.replace(\"_\", \" \").title()\n",
    "            print(f\"\\n‚úÖ Results for {display_name}:\")\n",
    "            print(f\"   ‚Ä¢ Test Accuracy: {metrics.get('accuracy', 0):.4f}\")\n",
    "            print(f\"   ‚Ä¢ Precision:     {metrics.get('precision', 0):.4f}\")\n",
    "            print(f\"   ‚Ä¢ Recall:        {metrics.get('recall', 0):.4f}\")\n",
    "            print(f\"   ‚Ä¢ F1 Score:      {metrics.get('f1', 0):.4f}\")\n",
    "            \n",
    "            # Check if results match training performance\n",
    "            expected_acc = 0.64  # From your training_parameters.json\n",
    "            actual_acc = metrics.get('accuracy', 0)\n",
    "            diff = abs(expected_acc - actual_acc)\n",
    "            \n",
    "            if diff < 0.05:\n",
    "                print(f\"   ‚úÖ Performance matches training! (difference: {diff:.4f})\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Performance differs from training by {diff:.4f}\")\n",
    "            \n",
    "            # Clean up\n",
    "            del model\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error evaluating {lora_folder}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Generate comparison report\n",
    "    if all_results:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìä FINAL COMPARISON REPORT\")\n",
    "        print('='*80)\n",
    "        \n",
    "        # Create comparison table\n",
    "        comparison_data = []\n",
    "        for folder_name, results in all_results.items():\n",
    "            metrics = results['metrics']\n",
    "            comparison_data.append({\n",
    "                'Model': folder_name.replace(\"_\", \" \").title(),\n",
    "                'Test Accuracy': metrics.get('accuracy', 0),\n",
    "                'Precision': metrics.get('precision', 0),\n",
    "                'Recall': metrics.get('recall', 0),\n",
    "                'F1 Score': metrics.get('f1', 0),\n",
    "                'Sanity Check': results.get('sanity_acc', 0),\n",
    "                'Avg Confidence': results.get('avg_confidence', 0)\n",
    "            })\n",
    "        \n",
    "        df_comparison = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Format for display\n",
    "        df_display = df_comparison.copy()\n",
    "        for col in ['Test Accuracy', 'Precision', 'Recall', 'F1 Score', 'Sanity Check', 'Avg Confidence']:\n",
    "            if col in df_display.columns:\n",
    "                df_display[col] = df_display[col].apply(lambda x: f\"{x:.4f}\")\n",
    "        \n",
    "        print(\"\\n\" + df_display.to_string(index=False))\n",
    "        \n",
    "        # Find best model\n",
    "        if len(comparison_data) > 1:\n",
    "            best_idx = df_comparison['Test Accuracy'].idxmax()\n",
    "            worst_idx = df_comparison['Test Accuracy'].idxmin()\n",
    "            \n",
    "            print(f\"\\nüèÜ Best model: {df_comparison.loc[best_idx, 'Model']} \"\n",
    "                  f\"(Accuracy: {df_comparison.loc[best_idx, 'Test Accuracy']:.4f})\")\n",
    "            print(f\"üìâ Worst model: {df_comparison.loc[worst_idx, 'Model']} \"\n",
    "                  f\"(Accuracy: {df_comparison.loc[worst_idx, 'Test Accuracy']:.4f})\")\n",
    "        \n",
    "        # Save results\n",
    "        csv_path = CHECKPOINTS_DIR / \"lora_evaluation_results_fixed.csv\"\n",
    "        df_comparison.to_csv(csv_path, index=False)\n",
    "        print(f\"\\nüíæ Results saved to: {csv_path}\")\n",
    "        \n",
    "        # Check for low performance\n",
    "        low_performance = df_comparison[df_comparison['Test Accuracy'] < 0.4]\n",
    "        if not low_performance.empty:\n",
    "            print(f\"\\n‚ö†Ô∏è  WARNING: Some models have low test accuracy:\")\n",
    "            for _, row in low_performance.iterrows():\n",
    "                print(f\"   ‚Ä¢ {row['Model']}: {row['Test Accuracy']:.4f}\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ All models performing as expected!\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation complete. {len(all_results)} models evaluated.\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No LoRA folders specified for evaluation.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY:\")\n",
    "print(f\"Models processed: {len(LORA_FOLDERS_TO_EVALUATE)}\")\n",
    "print(f\"Successfully evaluated: {len(all_results) if 'all_results' in locals() else 0}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1137e29b",
   "metadata": {},
   "source": [
    "### LoRa Debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa807c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from transformers import ViTForImageClassification\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define paths\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "LORA_FOLDER = \"lora_r4_light1\"\n",
    "LORA_ADAPTER_PATH = CHECKPOINTS_DIR / LORA_FOLDER / \"lora_adapter\"\n",
    "\n",
    "print(\"üîç DEBUGGING LoRA MODEL LOADING\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"LoRA folder: {LORA_FOLDER}\")\n",
    "print(f\"Adapter path: {LORA_ADAPTER_PATH}\")\n",
    "print(f\"Path exists: {LORA_ADAPTER_PATH.exists()}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Check what files exist\n",
    "print(\"\\nüìÑ Checking files in lora_adapter directory:\")\n",
    "if LORA_ADAPTER_PATH.exists():\n",
    "    for file in LORA_ADAPTER_PATH.iterdir():\n",
    "        print(f\"  ‚Ä¢ {file.name} (Size: {file.stat().st_size:,} bytes)\")\n",
    "else:\n",
    "    print(f\"‚ùå lora_adapter directory does not exist!\")\n",
    "\n",
    "# 2. Load base model and check its performance\n",
    "print(\"\\nüß™ Step 1: Testing BASE MODEL performance (should be random)\")\n",
    "base_model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=NUM_LABELS,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "# Quick test on a few samples\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        outputs = base_model(pixel_values=images)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Just test first batch\n",
    "        break\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "base_acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Base model accuracy on first batch: {base_acc:.4f}\")\n",
    "print(f\"Predictions distribution: {np.bincount(all_preds, minlength=NUM_LABELS)}\")\n",
    "print(f\"True labels distribution: {np.bincount(all_labels, minlength=NUM_LABELS)}\")\n",
    "\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3. Try loading LoRA with different methods\n",
    "print(\"\\nüß™ Step 2: Testing LoRA loading methods\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nMethod 1: Using load_lora_model from your src.lora\")\n",
    "    from src.lora import load_lora_model\n",
    "    \n",
    "    base_model = ViTForImageClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    lora_model = load_lora_model(\n",
    "        base_model=base_model,\n",
    "        lora_adapter_path=str(LORA_ADAPTER_PATH),\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Check if weights are actually trainable\n",
    "    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "    total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "    \n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable percentage: {(trainable_params/total_params)*100:.2f}%\")\n",
    "    \n",
    "    # Check a few specific LoRA parameters\n",
    "    print(\"\\nChecking specific LoRA parameter names and sizes:\")\n",
    "    for name, param in lora_model.named_parameters():\n",
    "        if 'lora' in name.lower() or any(x in name.lower() for x in ['lora_a', 'lora_b']):\n",
    "            print(f\"  {name}: {param.shape}, requires_grad: {param.requires_grad}\")\n",
    "            if param.requires_grad:\n",
    "                print(f\"    Mean: {param.data.mean().item():.6f}, Std: {param.data.std().item():.6f}\")\n",
    "        if len([name for name, _ in lora_model.named_parameters() if 'lora' in name.lower()]) > 10:\n",
    "            print(\"  ... (showing first 10 LoRA parameters)\")\n",
    "            break\n",
    "    \n",
    "    # Quick test\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            \n",
    "            outputs = lora_model(pixel_values=images)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            break\n",
    "    \n",
    "    lora_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nLoRA model accuracy on first batch: {lora_acc:.4f}\")\n",
    "    print(f\"Predictions distribution: {np.bincount(all_preds, minlength=NUM_LABELS)}\")\n",
    "    \n",
    "    del lora_model, base_model\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with Method 1: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 4. Check the adapter config\n",
    "print(\"\\nüìã Step 3: Checking adapter_config.json\")\n",
    "config_path = LORA_ADAPTER_PATH / \"adapter_config.json\"\n",
    "if config_path.exists():\n",
    "    import json\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"LoRA Configuration:\")\n",
    "    for key, value in config.items():\n",
    "        if key not in ['peft_type', 'task_type', 'inference_mode']:\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "# 5. Try loading directly with PeftModel\n",
    "print(\"\\nüß™ Step 4: Loading directly with PeftModel\")\n",
    "try:\n",
    "    base_model = ViTForImageClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=NUM_LABELS,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading from: {LORA_ADAPTER_PATH}\")\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        LORA_ADAPTER_PATH,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Loaded with PeftModel.from_pretrained\")\n",
    "    \n",
    "    # Check if it's actually a Peft model\n",
    "    print(f\"Is PeftModel instance: {isinstance(lora_model, PeftModel)}\")\n",
    "    \n",
    "    # Count trainable params\n",
    "    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Test inference\n",
    "    lora_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a single batch\n",
    "        test_sample = next(iter(test_loader))\n",
    "        images, labels = test_sample\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        # Test without merging\n",
    "        outputs = lora_model(pixel_values=images)\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        print(f\"\\nSingle batch predictions: {preds.cpu().numpy()[:10]}\")\n",
    "        \n",
    "        # Test with merging\n",
    "        print(\"\\nTesting with merged weights...\")\n",
    "        merged_model = lora_model.merge_and_unload()\n",
    "        outputs_merged = merged_model(pixel_values=images)\n",
    "        preds_merged = torch.argmax(outputs_merged.logits, dim=-1)\n",
    "        print(f\"Merged model predictions: {preds_merged.cpu().numpy()[:10]}\")\n",
    "        \n",
    "        # Check if predictions are different\n",
    "        if torch.equal(preds, preds_merged):\n",
    "            print(\"‚ö†Ô∏è WARNING: Predictions are identical (LoRA might not be applied)\")\n",
    "        else:\n",
    "            print(\"‚úì Predictions differ (LoRA is being applied)\")\n",
    "    \n",
    "    del lora_model, merged_model, base_model\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error with direct PeftModel loading: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 6. Check if weights were actually saved during training\n",
    "print(\"\\nüìä Step 5: Checking training history\")\n",
    "history_path = CHECKPOINTS_DIR / LORA_FOLDER / f\"history_{LORA_FOLDER}.json\"\n",
    "if history_path.exists():\n",
    "    import json\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "    \n",
    "    if 'val_acc' in history and history['val_acc']:\n",
    "        best_val_acc = max(history['val_acc'])\n",
    "        print(f\"Best validation accuracy during training: {best_val_acc:.4f}\")\n",
    "        print(f\"Final training accuracy: {history['train_acc'][-1] if 'train_acc' in history else 'N/A'}\")\n",
    "        \n",
    "        if best_val_acc < 0.2:\n",
    "            print(\"‚ö†Ô∏è WARNING: Model had poor accuracy during training too!\")\n",
    "        else:\n",
    "            print(\"‚úÖ Model trained well, but loading is the issue\")\n",
    "\n",
    "# 7. Check the .pth file\n",
    "print(\"\\nüì¶ Step 6: Checking .pth checkpoint\")\n",
    "pth_files = list((CHECKPOINTS_DIR / LORA_FOLDER).glob(\"*.pth\"))\n",
    "if pth_files:\n",
    "    pth_file = pth_files[0]\n",
    "    print(f\"Found .pth file: {pth_file.name} ({pth_file.stat().st_size:,} bytes)\")\n",
    "    \n",
    "    # Try loading it\n",
    "    try:\n",
    "        checkpoint = torch.load(pth_file, map_location='cpu', weights_only=True)\n",
    "        print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "        \n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            state_dict = checkpoint['model_state_dict']\n",
    "            print(f\"State dict keys (first 10): {list(state_dict.keys())[:10]}\")\n",
    "            \n",
    "            # Check for LoRA weights\n",
    "            lora_keys = [k for k in state_dict.keys() if 'lora' in k.lower()]\n",
    "            print(f\"Number of LoRA keys in .pth: {len(lora_keys)}\")\n",
    "            if lora_keys:\n",
    "                print(f\"Sample LoRA keys: {lora_keys[:5]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading .pth file: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üí° RECOMMENDED NEXT STEPS:\")\n",
    "print(\"1. Check if LoRA weights were actually saved during training\")\n",
    "print(\"2. Verify the adapter_config.json has correct settings\")\n",
    "print(\"3. Try loading with merge_and_unload() for inference\")\n",
    "print(\"4. Check if base model architecture matches LoRA training\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e799e73",
   "metadata": {},
   "source": [
    "#### LoRa Diagnostic Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15503de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CHECKPOINT INSPECTOR: See what's actually saved in your LoRA checkpoint\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "LORA_FOLDER = \"lora_r4_light1\"\n",
    "\n",
    "folder_path = CHECKPOINTS_DIR / LORA_FOLDER\n",
    "\n",
    "print(\"üîç Inspecting LoRA Checkpoint\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Folder: {LORA_FOLDER}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Check what files exist\n",
    "print(\"\\n1Ô∏è‚É£ Files in checkpoint folder:\")\n",
    "for item in folder_path.iterdir():\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024**2)\n",
    "        print(f\"   üìÑ {item.name} ({size_mb:.2f} MB)\")\n",
    "    elif item.is_dir():\n",
    "        print(f\"   üìÅ {item.name}/\")\n",
    "\n",
    "# 2. Inspect the .pth checkpoint\n",
    "pth_file = folder_path / \"best_lora_r4_light.pth\"\n",
    "if pth_file.exists():\n",
    "    print(f\"\\n2Ô∏è‚É£ Inspecting {pth_file.name}:\")\n",
    "    checkpoint = torch.load(pth_file, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    print(f\"\\n   Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "    \n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        state_dict = checkpoint['model_state_dict']\n",
    "        \n",
    "        print(f\"\\n   Total keys in state_dict: {len(state_dict)}\")\n",
    "        \n",
    "        # Categorize keys\n",
    "        lora_keys = [k for k in state_dict.keys() if 'lora' in k.lower()]\n",
    "        classifier_keys = [k for k in state_dict.keys() if 'classifier' in k.lower()]\n",
    "        other_keys = [k for k in state_dict.keys() if 'lora' not in k.lower() and 'classifier' not in k.lower()]\n",
    "        \n",
    "        print(f\"\\n   üìä Key categories:\")\n",
    "        print(f\"   - LoRA keys: {len(lora_keys)}\")\n",
    "        print(f\"   - Classifier keys: {len(classifier_keys)}\")\n",
    "        print(f\"   - Other keys: {len(other_keys)}\")\n",
    "        \n",
    "        if classifier_keys:\n",
    "            print(f\"\\n   ‚úÖ Classifier keys found:\")\n",
    "            for key in classifier_keys:\n",
    "                tensor = state_dict[key]\n",
    "                print(f\"   - {key}: shape={tensor.shape}, mean={tensor.mean().item():.6f}, std={tensor.std().item():.6f}\")\n",
    "        else:\n",
    "            print(f\"\\n   ‚ùå NO classifier keys found in checkpoint!\")\n",
    "        \n",
    "        if lora_keys:\n",
    "            print(f\"\\n   ‚úÖ Sample LoRA keys (first 5):\")\n",
    "            for key in lora_keys[:5]:\n",
    "                tensor = state_dict[key]\n",
    "                print(f\"   - {key}: shape={tensor.shape}\")\n",
    "        \n",
    "        # Check for signs of training\n",
    "        print(f\"\\n   üîç Checking if weights were actually trained...\")\n",
    "        \n",
    "        # Check classifier weights\n",
    "        if 'base_model.model.classifier.weight' in state_dict:\n",
    "            classifier_weight = state_dict['base_model.model.classifier.weight']\n",
    "            print(f\"   Classifier weight stats:\")\n",
    "            print(f\"   - Mean: {classifier_weight.mean().item():.6f}\")\n",
    "            print(f\"   - Std: {classifier_weight.std().item():.6f}\")\n",
    "            print(f\"   - Min: {classifier_weight.min().item():.6f}\")\n",
    "            print(f\"   - Max: {classifier_weight.max().item():.6f}\")\n",
    "            \n",
    "            # Check if it's close to random initialization\n",
    "            if abs(classifier_weight.mean().item()) < 0.01 and abs(classifier_weight.std().item() - 0.02) < 0.01:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: Classifier weights look like random initialization!\")\n",
    "        \n",
    "        # Check a LoRA weight\n",
    "        if lora_keys:\n",
    "            sample_lora = state_dict[lora_keys[0]]\n",
    "            print(f\"\\n   Sample LoRA weight stats ({lora_keys[0]}):\")\n",
    "            print(f\"   - Mean: {sample_lora.mean().item():.6f}\")\n",
    "            print(f\"   - Std: {sample_lora.std().item():.6f}\")\n",
    "    \n",
    "    if 'val_acc' in checkpoint:\n",
    "        print(f\"\\n   üìà Training metrics from checkpoint:\")\n",
    "        print(f\"   - Validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "        print(f\"   - Validation loss: {checkpoint.get('val_loss', 'N/A')}\")\n",
    "        print(f\"   - Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "\n",
    "# 3. Check lora_adapter folder\n",
    "lora_adapter_path = folder_path / \"lora_adapter\"\n",
    "if lora_adapter_path.exists():\n",
    "    print(f\"\\n3Ô∏è‚É£ Inspecting lora_adapter folder:\")\n",
    "    \n",
    "    # Check adapter_config.json\n",
    "    config_file = lora_adapter_path / \"adapter_config.json\"\n",
    "    if config_file.exists():\n",
    "        with open(config_file, 'r') as f:\n",
    "            adapter_config = json.load(f)\n",
    "        \n",
    "        print(f\"\\n   adapter_config.json:\")\n",
    "        print(f\"   - r: {adapter_config.get('r')}\")\n",
    "        print(f\"   - lora_alpha: {adapter_config.get('lora_alpha')}\")\n",
    "        print(f\"   - target_modules: {adapter_config.get('target_modules')}\")\n",
    "        print(f\"   - lora_dropout: {adapter_config.get('lora_dropout')}\")\n",
    "    \n",
    "    # Check adapter weights\n",
    "    adapter_files = list(lora_adapter_path.glob(\"*.safetensors\")) + list(lora_adapter_path.glob(\"*.bin\"))\n",
    "    if adapter_files:\n",
    "        print(f\"\\n   Adapter weight files:\")\n",
    "        for f in adapter_files:\n",
    "            size_mb = f.stat().st_size / (1024**2)\n",
    "            print(f\"   - {f.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        # Try to load adapter weights\n",
    "        try:\n",
    "            if adapter_files[0].suffix == '.safetensors':\n",
    "                from safetensors.torch import load_file\n",
    "                adapter_weights = load_file(str(adapter_files[0]))\n",
    "            else:\n",
    "                adapter_weights = torch.load(adapter_files[0], map_location='cpu')\n",
    "            \n",
    "            print(f\"\\n   Adapter weights:\")\n",
    "            print(f\"   - Total keys: {len(adapter_weights)}\")\n",
    "            print(f\"   - Sample keys (first 3):\")\n",
    "            for key in list(adapter_weights.keys())[:3]:\n",
    "                print(f\"     ‚Ä¢ {key}: {adapter_weights[key].shape}\")\n",
    "            \n",
    "            # Check if there are classifier weights in adapter\n",
    "            classifier_in_adapter = [k for k in adapter_weights.keys() if 'classifier' in k.lower()]\n",
    "            if classifier_in_adapter:\n",
    "                print(f\"\\n   ‚úÖ Classifier weights in adapter: {len(classifier_in_adapter)}\")\n",
    "            else:\n",
    "                print(f\"\\n   ‚ùå NO classifier weights in adapter!\")\n",
    "                print(f\"   This means the classifier was never saved with LoRA adapters!\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not load adapter weights: {e}\")\n",
    "\n",
    "# 4. Check training_parameters.json\n",
    "params_file = folder_path / \"training_parameters.json\"\n",
    "if params_file.exists():\n",
    "    print(f\"\\n4Ô∏è‚É£ Training parameters:\")\n",
    "    with open(params_file, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    print(f\"   - Trainable params: {params.get('trainable_params', 'N/A'):,}\")\n",
    "    print(f\"   - Total params: {params.get('total_params', 'N/A'):,}\")\n",
    "    print(f\"   - Trainable %: {params.get('trainable_percentage', 'N/A'):.4f}%\")\n",
    "    print(f\"   - Best val accuracy: {params.get('final_metrics', {}).get('best_val_accuracy', 'N/A')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä DIAGNOSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüí° Key findings:\")\n",
    "print(\"   1. Check if classifier keys exist in .pth checkpoint\")\n",
    "print(\"   2. Check if classifier keys exist in lora_adapter/\")\n",
    "print(\"   3. Compare trainable params from training vs what was saved\")\n",
    "print(\"\\n   If classifier is missing from lora_adapter/, that's your problem!\")\n",
    "print(\"   The LoRA adapters only save LoRA weights, not the classifier.\")\n",
    "print(\"   The classifier must be saved separately in the .pth checkpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37201a1",
   "metadata": {},
   "source": [
    "### Resume Training from Last Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Training from Last Backup\n",
    "from src.backup import resume_training\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "MODELS_TO_RESUME = [\n",
    "    \"baseline_heavy\",\n",
    "]\n",
    "\n",
    "for model_folder in MODELS_TO_RESUME:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Resuming: {model_folder}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        run_folder = CHECKPOINTS_DIR / model_folder\n",
    "        \n",
    "        # Load training parameters to get original settings\n",
    "        params_path = run_folder / \"training_parameters.json\"\n",
    "        with open(params_path, 'r') as f:\n",
    "            training_params = json.load(f)\n",
    "        \n",
    "        # Create fresh model and datasets\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\",\n",
    "            num_labels=7,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Determine transform\n",
    "        transform_key = \"none\"\n",
    "        if 'heavy' in model_folder.lower():\n",
    "            transform_key = \"heavy\"\n",
    "        elif 'medium' in model_folder.lower():\n",
    "            transform_key = \"medium\"\n",
    "        elif 'light' in model_folder.lower():\n",
    "            transform_key = \"light\"\n",
    "        \n",
    "        transform = transform_configs[transform_key]\n",
    "        \n",
    "        train_ds = FER2013Dataset(split=\"train\", transform=transform)\n",
    "        val_ds = FER2013Dataset(split=\"valid\", transform=base_transform())\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            model.parameters(), \n",
    "            lr=training_params['learning_rate'],\n",
    "            weight_decay=training_params['optimizer_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Resume training\n",
    "        model_resumed, history, new_run_folder = resume_training(\n",
    "            run_folder=run_folder,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train_ds,\n",
    "            val_dataset=val_ds,\n",
    "            num_epochs=training_params['num_epochs'], \n",
    "            batch_size=training_params['batch_size'],\n",
    "            device=\"cuda\",\n",
    "            model_name=f\"resumed_{model_folder}\",\n",
    "            use_wandb=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully resumed: {model_folder}\")\n",
    "        print(f\"New run folder: {new_run_folder}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to resume {model_folder}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion-Classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
