{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6402fff3",
   "metadata": {},
   "source": [
    "# Google ViT \n",
    "## Initalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6d4a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 0 \n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "from transformers import ViTForImageClassification\n",
    "import random\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from src.transforms import base_transform\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.config import (\n",
    "    DEVICE, \n",
    "    NUM_LABELS, \n",
    "    EMOTION_LABELS,\n",
    "    DEFAULT_BATCH_SIZE,\n",
    "    DEFAULT_LEARNING_RATE\n",
    ")\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from src.train import train_model\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "MODEL_NAME = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91481961",
   "metadata": {},
   "source": [
    "### Weights and Biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f21c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 \n",
    "from src.wandb_utils import login, check_wandb_mode, sync_offline_runs\n",
    "\n",
    "# \"online\", \"offline\", or \"disabled\"\n",
    "# If set to offlien dont forget to sink\n",
    "WANDB_MODE = \"online\" \n",
    "\n",
    "print(\"Initializing Weights & Biases...\")\n",
    "current_mode = login(\n",
    "    project=\"emotion-classifier-vit\",\n",
    "    mode=WANDB_MODE\n",
    ")\n",
    "\n",
    "print(f\"W&B initialized successfully in {current_mode.upper()} mode!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac93a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.wandb_utils import *\n",
    "\n",
    "# Weights and Biases Util Commands \n",
    "\n",
    "# Check current mode\n",
    "# check_wandb_mode()\n",
    "\n",
    "# Sync offline runs (when you have internet)\n",
    "sync_offline_runs(all_runs=True)\n",
    "\n",
    "# List available offline runs\n",
    "# list_offline_runs()\n",
    "\n",
    "# Change mode \n",
    "# set_wandb_mode(\"online\")  \n",
    "\n",
    "# Set Confirm to False for a Dry Run\n",
    "# clear_offline_runs(confirm=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9719f4",
   "metadata": {},
   "source": [
    "---\n",
    "##  Fine Tuning Section\n",
    "Using FER2013 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ebee14",
   "metadata": {},
   "source": [
    "### Tranformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bbce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simpler transformation sets without deprecated parameters\n",
    "transform_configs = {\n",
    "    \"none\": base_transform(),  # Use the base transforms from transforms.py\n",
    "    \n",
    "    \"light\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        A.Affine(translate_percent=0.05, scale=(0.95, 1.05), rotate=(-10, 10), p=0.3),\n",
    "        *base_transform()  # Include base transforms at the end\n",
    "    ]),\n",
    "    \n",
    "    \"medium\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Affine(translate_percent=0.1, scale=(0.9, 1.1), rotate=(-15, 15), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        *base_transform()  # Include base transforms at the end\n",
    "    ]),\n",
    "    \n",
    "    \"heavy\": A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "        A.Affine(translate_percent=0.15, scale=(0.85, 1.15), rotate=(-20, 20), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.4),\n",
    "        A.GridDropout(ratio=0.1, p=0.3),\n",
    "        *base_transform()  # Include base transforms at the end\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Transformation Configs Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7e6141",
   "metadata": {},
   "source": [
    "### Hyper Parameter Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3192cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configurations\n",
    "EPOCHS = 6\n",
    "\n",
    "experiment_configs = [\n",
    "    # Baseline with different transforms\n",
    "    {\n",
    "        \"name\": \"baseline_none\",\n",
    "        \"transform_key\": \"none\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_light\",\n",
    "        \"transform_key\": \"light\", \n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_medium\",\n",
    "        \"transform_key\": \"medium\",\n",
    "        \"epochs\": EPOCHS, \n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE,\n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"baseline_heavy\",\n",
    "        \"transform_key\": \"heavy\",\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"learning_rate\": DEFAULT_LEARNING_RATE, \n",
    "        \"batch_size\": DEFAULT_BATCH_SIZE,\n",
    "        \"weight_decay\": 0.01\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"{len(experiment_configs)} Experiment Configs Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9b0475",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from src.wandb_utils import cleanup_wandb_run\n",
    "\n",
    "all_results = {}\n",
    "failed_experiments = []\n",
    "\n",
    "print(f\"Starting training for {len(experiment_configs)} experiments\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, config in enumerate(tqdm(experiment_configs, desc=\"Training Experiments\")):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üî¨ Experiment {i+1}/{len(experiment_configs)}: {config['name']}\")\n",
    "    print(f\"   Transform: {config['transform_key']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}\")\n",
    "    print(f\"   Epochs: {config['epochs']}\")\n",
    "    print(f\"   Batch Size: {config['batch_size']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Ensure any previous WandB run is cleaned up\n",
    "    cleanup_wandb_run()\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        transform = transform_configs[config['transform_key']]\n",
    "        \n",
    "        train = FER2013Dataset(\n",
    "            split=\"train\",\n",
    "            transform=transform\n",
    "        )\n",
    "        valid = FER2013Dataset(\n",
    "            split=\"valid\", \n",
    "            transform=base_transform()  \n",
    "        )\n",
    "        \n",
    "        # Initialize model\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            num_labels=NUM_LABELS,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Initialize optimizer\n",
    "        optimizer = AdamW(\n",
    "            model.parameters(), \n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "        \n",
    "        # Train model\n",
    "        model_exp, history_exp, run_folder_exp = train_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train,\n",
    "            val_dataset=valid,\n",
    "            num_epochs=config['epochs'],\n",
    "            batch_size=config['batch_size'],\n",
    "            device=DEVICE,\n",
    "            model_name=config['name'],  \n",
    "            use_wandb=True,\n",
    "            wandb_config={\n",
    "                \"learning_rate\": config['learning_rate'],\n",
    "                \"batch_size\": config['batch_size'],\n",
    "                \"epochs\": config['epochs'],\n",
    "                \"weight_decay\": config['weight_decay'],\n",
    "                \"model_name\": \"vit_base_patch16_224\",\n",
    "                \"architecture\": \"ViT\", \n",
    "                \"dataset\": \"FER2013\",\n",
    "                \"transform_set\": config['transform_key'],\n",
    "                \"experiment_name\": config['name']\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[config['name']] = {\n",
    "            'model': model_exp,\n",
    "            'history': history_exp,\n",
    "            'run_folder': run_folder_exp,\n",
    "            'config': config,\n",
    "            'best_val_accuracy': max(history_exp['val_acc']),      \n",
    "            'best_val_loss': min(history_exp['val_loss']),\n",
    "            'final_train_accuracy': history_exp['train_acc'][-1],  \n",
    "            'final_train_loss': history_exp['train_loss'][-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n COMPLETED: {config['name']}\")\n",
    "        print(f\"   Best Val Accuracy: {all_results[config['name']]['best_val_accuracy']:.4f}\")\n",
    "        print(f\"   Best Val Loss: {all_results[config['name']]['best_val_loss']:.4f}\")\n",
    "        print(f\"   Run folder: {run_folder_exp}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n  Training interrupted by user at experiment: {config['name']}\")\n",
    "        cleanup_wandb_run()\n",
    "        break\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n ERROR in experiment {config['name']}: {str(e)}\")\n",
    "        print(f\"   Exception type: {type(e).__name__}\")\n",
    "        \n",
    "        # Store failed experiment info\n",
    "        failed_experiments.append({\n",
    "            'name': config['name'],\n",
    "            'error': str(e),\n",
    "            'error_type': type(e).__name__\n",
    "        })\n",
    "        \n",
    "        # Clean up WandB\n",
    "        cleanup_wandb_run()\n",
    "        \n",
    "        # Decide whether to continue or stop\n",
    "        print(f\"   Continuing to next experiment...\")\n",
    "        \n",
    "    finally:\n",
    "        # Clean up memory regardless of success/failure\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'model_exp' in locals():\n",
    "            del model_exp\n",
    "        if 'optimizer' in locals():\n",
    "            del optimizer\n",
    "        if 'train' in locals():\n",
    "            del train\n",
    "        if 'valid' in locals():\n",
    "            del valid\n",
    "            \n",
    "        # Force garbage collection\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        print(f\"   Memory cleaned up\")\n",
    "\n",
    "# Final cleanup\n",
    "cleanup_wandb_run()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\" Successful experiments: {len(all_results)}/{len(experiment_configs)}\")\n",
    "print(f\" Failed experiments: {len(failed_experiments)}/{len(experiment_configs)}\")\n",
    "\n",
    "if all_results:\n",
    "    print(\"\\n Results:\")\n",
    "    for name, result in all_results.items():\n",
    "        print(f\"   {name}: Val Acc = {result['best_val_accuracy']:.4f}, Val Loss = {result['best_val_loss']:.4f}\")\n",
    "\n",
    "if failed_experiments:\n",
    "    print(\"\\n  Failed Experiments:\")\n",
    "    for failed in failed_experiments:\n",
    "        print(f\"   {failed['name']}: {failed['error_type']} - {failed['error']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f569b0",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438d42c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Independent evaluation (can run after kernel restart)\n",
    "from src.evaluate import evaluate_all_saved_models\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß™ Starting INDEPENDENT evaluation of all saved models...\")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate all saved models (no need for all_results in memory)\n",
    "summary_data = evaluate_all_saved_models(test_ds)\n",
    "\n",
    "print(\"\\n‚úÖ All saved models evaluated and summarized!\")\n",
    "print(f\"üìä Performance plot saved to: experiment_performance_comparison.png\")\n",
    "\n",
    "# Show best model details\n",
    "if summary_data:\n",
    "    best_exp = summary_data[0]\n",
    "    print(f\"\\nüèÜ Best model: {best_exp['experiment']}\")\n",
    "    print(f\"   Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "    print(f\"   Transform: {best_exp['transform']}\")\n",
    "    print(f\"   Run Folder: {best_exp['run_folder']}\")\n",
    "else:\n",
    "    print(\"‚ùå No models were successfully evaluated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9A: Evaluate specific experiments using your experiment_configs\n",
    "from src.evaluate import evaluate_from_experiment_configs\n",
    "from src.dataset import FER2013Dataset\n",
    "from src.transforms import base_transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"üß™ Evaluating specific experiments from config...\")\n",
    "\n",
    "# Load test dataset\n",
    "test_ds = FER2013Dataset(\n",
    "    split=\"test\", \n",
    "    transform=base_transform()\n",
    ")\n",
    "\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "\n",
    "# Evaluate using your experiment_configs (finds latest runs automatically)\n",
    "summary_data = evaluate_from_experiment_configs(experiment_configs, test_ds)\n",
    "\n",
    "print(\"\\n‚úÖ Specific experiments evaluated!\")\n",
    "print(f\"üìä Performance plot saved to: experiment_performance_comparison.png\")\n",
    "\n",
    "# Show best model details\n",
    "if summary_data:\n",
    "    best_exp = summary_data[0]\n",
    "    print(f\"\\nüèÜ Best model: {best_exp['experiment']}\")\n",
    "    print(f\"   Run: {best_exp['run_name']}\")\n",
    "    print(f\"   Test Accuracy: {best_exp['test_accuracy']:.4f}\")\n",
    "    print(f\"   Transform: {best_exp['transform']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbae024",
   "metadata": {},
   "source": [
    "---\n",
    "##  Test Predictions\n",
    "Let's visualize some predictions from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc32f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions from multiple models\n",
    "from src.metadata import find_latest_run_for_experiment, load_training_parameters\n",
    "from src.checkpoint_utils import load_model_from_checkpoint\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "MODELS_TO_TEST = [\n",
    "    \"baseline_none\",\n",
    "    \"baseline_light\", \n",
    "]\n",
    "\n",
    "NUM_SAMPLES = 3  # Number of random test samples per model\n",
    "\n",
    "def predict_and_visualize_batch(dataset, indices, model, processor, model_name):\n",
    "    \"\"\"Run predictions on multiple samples for a single model.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img, true_label = dataset[idx]\n",
    "        img_pil = transforms.ToPILImage()(img)\n",
    "        \n",
    "        # Run model\n",
    "        model.eval()\n",
    "        model.to(DEVICE)\n",
    "        inputs = processor(images=img_pil, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Post-process\n",
    "        probs = torch.softmax(outputs.logits, dim=-1)[0]\n",
    "        pred_label = torch.argmax(probs).item()\n",
    "        confidence = probs[pred_label].item()\n",
    "        \n",
    "        # Get top 3 predictions\n",
    "        top3_probs, top3_idx = torch.topk(probs, 3)\n",
    "        top3_predictions = [\n",
    "            (EMOTION_LABELS[idx.item()], prob.item()) \n",
    "            for prob, idx in zip(top3_probs, top3_idx)\n",
    "        ]\n",
    "        \n",
    "        results.append({\n",
    "            'sample_index': idx,\n",
    "            'true_label': true_label,\n",
    "            'pred_label': pred_label,\n",
    "            'confidence': confidence,\n",
    "            'correct': true_label == pred_label,\n",
    "            'top3_predictions': top3_predictions,\n",
    "            'image': img_pil\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def display_model_predictions(model_name, results, sample_indices):\n",
    "    \"\"\"Display predictions for a single model.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    correct_count = sum(1 for r in results if r['correct'])\n",
    "    accuracy = correct_count / len(results)\n",
    "    \n",
    "    print(f\"Batch Accuracy: {correct_count}/{len(results)} ({accuracy:.1%})\")\n",
    "    print(f\"Samples tested: {sample_indices}\")\n",
    "    print()\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        print(f\"Sample {i+1} (Index {result['sample_index']}):\")\n",
    "        print(f\"  True: {EMOTION_LABELS[result['true_label']]:<12}\", end=\"\")\n",
    "        print(f\"  Predicted: {EMOTION_LABELS[result['pred_label']]:<12}\", end=\"\")\n",
    "        print(f\"  Confidence: {result['confidence']:.1%}\", end=\"\")\n",
    "        print(f\"  {'‚úì' if result['correct'] else '‚úó'}\")\n",
    "        \n",
    "        # Show top 3 predictions\n",
    "        print(f\"  Top 3: \", end=\"\")\n",
    "        for j, (emotion, prob) in enumerate(result['top3_predictions']):\n",
    "            print(f\"{emotion}: {prob:.1%}\", end=\"\")\n",
    "            if j < 2:\n",
    "                print(\", \", end=\"\")\n",
    "        print()\n",
    "    \n",
    "    # Visualize all samples in a grid\n",
    "    fig, axes = plt.subplots(1, len(results), figsize=(4*len(results), 4))\n",
    "    if len(results) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (result, ax) in enumerate(zip(results, axes)):\n",
    "        ax.imshow(result['image'], cmap='gray')\n",
    "        correct = result['correct']\n",
    "        color = 'green' if correct else 'red'\n",
    "        title = f\"Sample {i+1}\\n\"\n",
    "        title += f\"True: {EMOTION_LABELS[result['true_label']]}\\n\"\n",
    "        title += f\"Pred: {EMOTION_LABELS[result['pred_label']]}\\n\"\n",
    "        title += f\"Conf: {result['confidence']:.1%}\"\n",
    "        ax.set_title(title, color=color, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Model: {model_name} (Accuracy: {accuracy:.1%})\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Load processor (same for all models)\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "print(f\"Testing {len(MODELS_TO_TEST)} models on {NUM_SAMPLES} random samples each\")\n",
    "print(f\"Test dataset size: {len(test_ds)}\")\n",
    "print()\n",
    "\n",
    "# Get random sample indices (same for all models for fair comparison)\n",
    "sample_indices = random.sample(range(len(test_ds)), NUM_SAMPLES)\n",
    "print(f\"Random sample indices: {sample_indices}\")\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for model_name in MODELS_TO_TEST:\n",
    "    try:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Loading model: {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Find the latest run for this model\n",
    "        run_folder = find_latest_run_for_experiment(model_name, CHECKPOINTS_DIR)\n",
    "        \n",
    "        # Load the best model checkpoint\n",
    "        checkpoint_path = run_folder / f\"best_{run_folder.name}.pth\"\n",
    "        \n",
    "        if not checkpoint_path.exists():\n",
    "            print(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Load model\n",
    "        model = load_model_from_checkpoint(checkpoint_path)\n",
    "        \n",
    "        # Get model info\n",
    "        params = load_training_parameters(run_folder)\n",
    "        print(f\"Loaded: {run_folder.name}\")\n",
    "        print(f\"Transform: {model_name.split('_')[-1]}\")\n",
    "        print(f\"Epochs: {params.get('num_epochs', 'N/A')}\")\n",
    "        print(f\"Learning rate: {params.get('learning_rate', 'N/A'):.2e}\")\n",
    "        \n",
    "        # Run predictions\n",
    "        results = predict_and_visualize_batch(\n",
    "            dataset=test_ds,\n",
    "            indices=sample_indices,\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # Display results\n",
    "        accuracy = display_model_predictions(model_name, results, sample_indices)\n",
    "        model_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'correct': sum(1 for r in results if r['correct']),\n",
    "            'total': len(results),\n",
    "            'run_folder': run_folder.name\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to test {model_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Print summary comparison\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"SUMMARY: Model Comparison\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if model_results:\n",
    "    # Sort by accuracy\n",
    "    sorted_results = sorted(\n",
    "        model_results.items(), \n",
    "        key=lambda x: x[1]['accuracy'], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nPerformance Ranking:\")\n",
    "    for i, (model_name, result) in enumerate(sorted_results):\n",
    "        print(f\"{i+1}. {model_name:<20} {result['correct']}/{result['total']} ({result['accuracy']:.1%})\")\n",
    "    \n",
    "    # Best and worst performers\n",
    "    best_model = sorted_results[0]\n",
    "    worst_model = sorted_results[-1]\n",
    "    \n",
    "    print(f\"\\nBest: {best_model[0]} ({best_model[1]['accuracy']:.1%})\")\n",
    "    print(f\"Worst: {worst_model[0]} ({worst_model[1]['accuracy']:.1%})\")\n",
    "    \n",
    "    # Optional: Create comparison visualization\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    models = [m[0] for m in sorted_results]\n",
    "    accuracies = [m[1]['accuracy'] for m in sorted_results]\n",
    "    \n",
    "    bars = ax.bar(models, accuracies, color=['green', 'lightgreen', 'orange', 'red'])\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Model Comparison on {NUM_SAMPLES} Samples')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, height + 0.02,\n",
    "                f'{acc:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No models were successfully tested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462663a",
   "metadata": {},
   "source": [
    "## Debug \n",
    "### Reload Failed Models from Backup Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144ddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume Training from Last Backup\n",
    "from src.backup import resume_training\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "CHECKPOINTS_DIR = Path(\"C:/Users/rayrc/OneDrive/Documents/ML/Emotion Classifier ViT/checkpoints\")\n",
    "\n",
    "MODELS_TO_RESUME = [\n",
    "    \"baseline_heavy\",\n",
    "]\n",
    "\n",
    "for model_folder in MODELS_TO_RESUME:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Resuming: {model_folder}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    try:\n",
    "        run_folder = CHECKPOINTS_DIR / model_folder\n",
    "        \n",
    "        # Load training parameters to get original settings\n",
    "        params_path = run_folder / \"training_parameters.json\"\n",
    "        with open(params_path, 'r') as f:\n",
    "            training_params = json.load(f)\n",
    "        \n",
    "        # Create fresh model and datasets\n",
    "        model = ViTForImageClassification.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\",\n",
    "            num_labels=7,\n",
    "            ignore_mismatched_sizes=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Determine transform\n",
    "        transform_key = \"none\"\n",
    "        if 'heavy' in model_folder.lower():\n",
    "            transform_key = \"heavy\"\n",
    "        elif 'medium' in model_folder.lower():\n",
    "            transform_key = \"medium\"\n",
    "        elif 'light' in model_folder.lower():\n",
    "            transform_key = \"light\"\n",
    "        \n",
    "        transform = transform_configs[transform_key]\n",
    "        \n",
    "        train_ds = FER2013Dataset(split=\"train\", transform=transform)\n",
    "        val_ds = FER2013Dataset(split=\"valid\", transform=base_transform())\n",
    "        \n",
    "        optimizer = AdamW(\n",
    "            model.parameters(), \n",
    "            lr=training_params['learning_rate'],\n",
    "            weight_decay=training_params['optimizer_params']['weight_decay']\n",
    "        )\n",
    "        \n",
    "        # Resume training\n",
    "        model_resumed, history, new_run_folder = resume_training(\n",
    "            run_folder=run_folder,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            train_dataset=train_ds,\n",
    "            val_dataset=val_ds,\n",
    "            num_epochs=training_params['num_epochs'], \n",
    "            batch_size=training_params['batch_size'],\n",
    "            device=\"cuda\",\n",
    "            model_name=f\"resumed_{model_folder}\",\n",
    "            use_wandb=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully resumed: {model_folder}\")\n",
    "        print(f\"New run folder: {new_run_folder}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to resume {model_folder}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Emotion Classifier ViT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
